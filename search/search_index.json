{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dados do M\u00f3dulo Agosto \u00b6 Data Sourcing e Data Analysis com R Studio Arquitetura e Governan\u00e7a de Dados na Nuvem 1 Arquitetura e Governan\u00e7a de Dados na Nuvem 2 Ingest\u00e3o e Processamento de Dados (Framework de testes e qualidade) Seguran\u00e7a e Governan\u00e7a de Dados (LGPD) Precifica\u00e7\u00e3o em Nuvem Setembro \u00b6 M\u00e9tricas, Telemetria e Gest\u00e3o de Microservi\u00e7os Data Lakes e Data Warehouses Servi\u00e7os e ETLs para Transforma\u00e7\u00e3o de Dados e Processamento de Dados em Escala Processamento de Dados na Nuvem Ciclo de Vida e Automatiza\u00e7\u00e3o de Pipeline de Dados (MLOps) Processamento e Predi\u00e7\u00e3o em Produ\u00e7\u00e3o Outubro \u00b6 User Interface e Dashboards Data Storytelling Boas Pr\u00e1ticas para o M\u00f3dulo \u00b6 Iremos utilizar fortemente o GitFlow Sugiro que voc\u00eas utilizem o GitGraph para VSCode Sugiro que o VSCode seja nosso ambiente principal de codifica\u00e7\u00e3o Instale o Pyhton 3.12 e sete como global Instale o code tools na op\u00e7\u00e3o de instala\u00e7\u00e3o se for por windows Vamos utilizar muito o Powershell no windows, se acostume! Desinstale o Github Desktop! \u00c9 s\u00e9rio... Tenha instalado o Docker e o Docker Compose Instale o R em sua \u00faltima vers\u00e3o e o RStudio Padronize seu nickname do git no seu computador desta forma: git config --global user.name \"Seu Nome\" git config --global user.email \"seu_email@exemplo.com\" Mantendo o mesmo username e email em todo o m\u00f3dulo! ou instale o gh ou github cli M\u00f3dulo 11 de Engenharia de Software: An\u00e1lise de Dados com RStudio \u00b6 Introdu\u00e7\u00e3o \u00b6 Nesta aula, vamos explorar a an\u00e1lise de dados avan\u00e7ada utilizando o RStudio e diversas ferramentas de suporte. Nosso parceiro, o Boston Consulting Group (BCG), fornecer\u00e1 arquivos CSV de grande porte para an\u00e1lise. Vamos abordar desde a padroniza\u00e7\u00e3o e normaliza\u00e7\u00e3o dos dados at\u00e9 a constru\u00e7\u00e3o de uma aplica\u00e7\u00e3o de an\u00e1lise de dados utilizando diversas tecnologias. Utilizaremos o conceito de processamento de dados \"Raw\", \"Working\" e \"Trusted\". Arquitetura e Ferramentas \u00b6 Ferramentas Utilizadas \u00b6 RStudio \u00b6 Descri\u00e7\u00e3o : Ambiente de desenvolvimento integrado (IDE) para a linguagem de programa\u00e7\u00e3o R. Uso : An\u00e1lise de dados, gera\u00e7\u00e3o de relat\u00f3rios e visualiza\u00e7\u00f5es. Mais detalhes : RStudio \u00e9 uma IDE poderosa para R que facilita a an\u00e1lise de dados, a cria\u00e7\u00e3o de gr\u00e1ficos e a gera\u00e7\u00e3o de relat\u00f3rios. Ele oferece uma interface amig\u00e1vel e ferramentas integradas para ajudar na codifica\u00e7\u00e3o, depura\u00e7\u00e3o e visualiza\u00e7\u00e3o de dados. RMarkdown \u00b6 Descri\u00e7\u00e3o : Ferramenta para criar documentos din\u00e2micos com R. Uso : Gera\u00e7\u00e3o de relat\u00f3rios para cada arquivo CSV analisado. Mais detalhes : RMarkdown permite a cria\u00e7\u00e3o de documentos din\u00e2micos que combinam c\u00f3digo R, texto e visualiza\u00e7\u00f5es. \u00c9 ideal para gerar relat\u00f3rios que podem ser facilmente atualizados com novos dados. Appwrite \u00b6 Descri\u00e7\u00e3o : Plataforma de Backend as a Service (BaaS) que oferece servi\u00e7os como autentica\u00e7\u00e3o, banco de dados, armazenamento e muito mais. Uso : Armazenamento de dados raw e dados processados, utilizando o storage e MariaDB. Mais detalhes : Appwrite \u00e9 uma plataforma BaaS que simplifica o desenvolvimento de aplicativos ao fornecer servi\u00e7os essenciais como autentica\u00e7\u00e3o, banco de dados, armazenamento de arquivos e muito mais, tudo em uma \u00fanica solu\u00e7\u00e3o. MariaDB \u00b6 Descri\u00e7\u00e3o : Sistema de gerenciamento de banco de dados relacional. Uso : Armazenamento de dados processados (dados working). Mais detalhes : MariaDB \u00e9 um sistema de gerenciamento de banco de dados relacional que \u00e9 uma bifurca\u00e7\u00e3o do MySQL. Ele \u00e9 conhecido por sua escalabilidade, desempenho e robustez, sendo amplamente utilizado para armazenar dados estruturados. ElasticSearch \u00b6 Descri\u00e7\u00e3o : Motor de busca e an\u00e1lise distribu\u00eddo e de c\u00f3digo aberto. Uso : Indexa\u00e7\u00e3o e busca avan\u00e7ada dos dados processados. Mais detalhes : ElasticSearch \u00e9 uma ferramenta poderosa para busca e an\u00e1lise de dados em tempo real. Ele permite a indexa\u00e7\u00e3o r\u00e1pida e eficiente de grandes volumes de dados, facilitando a busca e a an\u00e1lise avan\u00e7ada. ClickHouse \u00b6 Descri\u00e7\u00e3o : Sistema de gerenciamento de banco de dados orientado a colunas, de c\u00f3digo aberto, para consultas anal\u00edticas em tempo real. Uso : Armazenamento e an\u00e1lise de dados processados (dados trusted). Mais detalhes : ClickHouse \u00e9 um sistema de banco de dados orientado a colunas que \u00e9 otimizado para consultas anal\u00edticas em tempo real. Ele \u00e9 ideal para grandes volumes de dados e oferece desempenho excepcional para an\u00e1lises complexas. Streamlit \u00b6 Descri\u00e7\u00e3o : Framework de c\u00f3digo aberto para a cria\u00e7\u00e3o de aplica\u00e7\u00f5es web interativas em Python. Uso : Constru\u00e7\u00e3o de uma aplica\u00e7\u00e3o de an\u00e1lise de dados (DataApp). Mais detalhes : Streamlit \u00e9 um framework que facilita a cria\u00e7\u00e3o de aplica\u00e7\u00f5es web interativas para an\u00e1lise de dados. Com ele, \u00e9 poss\u00edvel transformar scripts Python em aplicativos web de forma r\u00e1pida e eficiente. Apache Spark \u00b6 Descri\u00e7\u00e3o : Motor de an\u00e1lise unificada para processamento de dados em grande escala. Uso : Processamento e predi\u00e7\u00e3o de dados em produ\u00e7\u00e3o. Mais detalhes : Apache Spark \u00e9 uma plataforma de processamento de dados em grande escala que oferece suporte para processamento em lote e em tempo real. Ele \u00e9 amplamente utilizado para an\u00e1lise de big data e aprendizado de m\u00e1quina. Docker \u00b6 Descri\u00e7\u00e3o : Plataforma para desenvolvimento, envio e execu\u00e7\u00e3o de aplica\u00e7\u00f5es em cont\u00eaineres. Uso : Dockeriza\u00e7\u00e3o da aplica\u00e7\u00e3o Streamlit, ElasticSearch e ClickHouse. Mais detalhes : Docker \u00e9 uma plataforma que permite a cria\u00e7\u00e3o, envio e execu\u00e7\u00e3o de aplica\u00e7\u00f5es em cont\u00eaineres. Ele facilita a portabilidade e a escalabilidade das aplica\u00e7\u00f5es, garantindo que elas funcionem de maneira consistente em diferentes ambientes. Poetry \u00b6 Descri\u00e7\u00e3o : Gestor de pacotes e depend\u00eancias para projetos Python. Uso : Gerenciamento de depend\u00eancias da aplica\u00e7\u00e3o Streamlit. Mais detalhes : Poetry \u00e9 uma ferramenta para gerenciamento de depend\u00eancias e pacotes em projetos Python. Ele simplifica a instala\u00e7\u00e3o e atualiza\u00e7\u00e3o de bibliotecas, garantindo que todas as depend\u00eancias sejam resolvidas corretamente. Portainer \u00b6 Descri\u00e7\u00e3o : Interface de gerenciamento para Docker. Uso : Gerenciamento de cont\u00eaineres Docker. Mais detalhes : Portainer \u00e9 uma interface de usu\u00e1rio para gerenciamento de cont\u00eaineres Docker. Ele facilita a administra\u00e7\u00e3o de ambientes Docker, permitindo a visualiza\u00e7\u00e3o, cria\u00e7\u00e3o e gerenciamento de cont\u00eaineres de forma intuitiva. Descri\u00e7\u00e3o : Interface de gerenciamento para Docker. Uso : Gerenciamento de cont\u00eaineres Docker. Mais detalhes Portainer \u00e9 uma interface de usu\u00e1rio para gerenciamento de cont\u00eaineres Docker. Ele facilita a administra\u00e7\u00e3o de ambientes Docker, permitindo a visualiza\u00e7\u00e3o, cria\u00e7\u00e3o e gerenciamento de cont\u00eaineres de forma intuitiva. Arquitetura de Processamento de Dados \u00b6 Recebimento dos Arquivos CSV - Os arquivos CSV fornecidos pelo BCG ser\u00e3o carregados no RStudio para an\u00e1lise inicial. An\u00e1lise e Padroniza\u00e7\u00e3o dos Dados - Utilizando RStudio, os dados ser\u00e3o ajustados e padronizados. - Relat\u00f3rios ser\u00e3o gerados em RMarkdown para cada arquivo CSV. Armazenamento dos Dados Raw - Os arquivos CSV originais ser\u00e3o armazenados no storage do Appwrite. Normaliza\u00e7\u00e3o e Processamento dos Dados - Os dados ser\u00e3o normalizados e processados no RStudio. - Dados processados (dados working) ser\u00e3o armazenados no MariaDB do Appwrite. Indexa\u00e7\u00e3o e Armazenamento de Dados Trusted - Um cont\u00eainer Docker do ElasticSearch ser\u00e1 configurado para indexa\u00e7\u00e3o e busca avan\u00e7ada. - Alternativamente, um cont\u00eainer Docker do ClickHouse ser\u00e1 configurado para armazenamento e an\u00e1lise de dados processados. - Dados do MariaDB ser\u00e3o indexados no ElasticSearch ou armazenados no ClickHouse. Constru\u00e7\u00e3o da Aplica\u00e7\u00e3o DataApp - Utilizando Streamlit, uma aplica\u00e7\u00e3o interativa ser\u00e1 desenvolvida para consumir dados do MariaDB, ElasticSearch e ClickHouse. - A aplica\u00e7\u00e3o ser\u00e1 dockerizada e gerenciada com Poetry. Processamento com Apache Spark - Apache Spark ser\u00e1 utilizado para realizar predi\u00e7\u00f5es com os dados em produ\u00e7\u00e3o. O que \u00e9 Backend as a Service (BaaS)? \u00b6 Backend as a Service (BaaS) \u00e9 um modelo de servi\u00e7o que fornece uma infraestrutura de backend pronta para uso, permitindo que desenvolvedores se concentrem no desenvolvimento do frontend e na l\u00f3gica de neg\u00f3cios. O BaaS oferece servi\u00e7os como autentica\u00e7\u00e3o, banco de dados, armazenamento de arquivos, notifica\u00e7\u00f5es push, entre outros, eliminando a necessidade de gerenciar servidores e infraestrutura. ElasticSearch vs ClickHouse \u00b6 ElasticSearch \u00b6 Descri\u00e7\u00e3o : Motor de busca e an\u00e1lise distribu\u00eddo, ideal para indexa\u00e7\u00e3o e busca avan\u00e7ada. Vantagens : - Excelente para buscas textuais e an\u00e1lises em tempo real. - Suporte robusto para consultas complexas e agrega\u00e7\u00f5es. Desvantagens : - Pode ser mais complexo de configurar e gerenciar para grandes volumes de dados. ClickHouse \u00b6 Descri\u00e7\u00e3o : Sistema de gerenciamento de banco de dados orientado a colunas, otimizado para consultas anal\u00edticas em tempo real. Vantagens : - Desempenho extremamente r\u00e1pido para consultas anal\u00edticas. - Otimizado para grandes volumes de dados e consultas de leitura intensiva. Desvantagens : - Menos adequado para buscas textuais complexas comparado ao ElasticSearch. Toda a arquitetura ser\u00e1 criada com Docker e rodar\u00e1 on-premise, mas poder\u00e1 ser facilmente integrada em uma nuvem ajustando os ambientes de conex\u00e3o (envs). Ferramentas e Tecnologias \u00b6 Ferramenta Descri\u00e7\u00e3o Uso RStudio IDE para R An\u00e1lise de dados, gera\u00e7\u00e3o de relat\u00f3rios RMarkdown Ferramenta para documentos din\u00e2micos Gera\u00e7\u00e3o de relat\u00f3rios Appwrite Plataforma de BaaS Armazenamento de dados raw e processados MariaDB Sistema de gerenciamento de banco de dados relacional Armazenamento de dados processados ElasticSearch Motor de busca e an\u00e1lise distribu\u00eddo Indexa\u00e7\u00e3o e busca avan\u00e7ada dos dados ClickHouse Sistema de gerenciamento de banco de dados orientado a colunas Armazenamento e an\u00e1lise de dados processados Streamlit Framework para cria\u00e7\u00e3o de aplica\u00e7\u00f5es web interativas Constru\u00e7\u00e3o de uma aplica\u00e7\u00e3o de an\u00e1lise de dados (DataApp) Apache Spark Motor de an\u00e1lise unificada para processamento de dados em grande escala Processamento e predi\u00e7\u00e3o de dados Docker Plataforma para desenvolvimento, envio e execu\u00e7\u00e3o de aplica\u00e7\u00f5es em cont\u00eaineres Dockeriza\u00e7\u00e3o da aplica\u00e7\u00e3o Streamlit, ElasticSearch e ClickHouse Poetry Gestor de pacotes e depend\u00eancias para projetos Python Gerenciamento de depend\u00eancias da aplica\u00e7\u00e3o Streamlit Mas isso n\u00e3o \u00e9 ferro e fogo! \u00b6 Sinta-se a vontade para trazer novas tecnologias para o nosso m\u00f3dulo! Conte com os professores para analisarmos juntos novas solu\u00e7\u00f5es. Aqui vai o refactoring Guru para dar novas ideias \u00e0 voc\u00eas =) Refactoring Guru","title":"Home"},{"location":"#agosto","text":"Data Sourcing e Data Analysis com R Studio Arquitetura e Governan\u00e7a de Dados na Nuvem 1 Arquitetura e Governan\u00e7a de Dados na Nuvem 2 Ingest\u00e3o e Processamento de Dados (Framework de testes e qualidade) Seguran\u00e7a e Governan\u00e7a de Dados (LGPD) Precifica\u00e7\u00e3o em Nuvem","title":"Agosto"},{"location":"#setembro","text":"M\u00e9tricas, Telemetria e Gest\u00e3o de Microservi\u00e7os Data Lakes e Data Warehouses Servi\u00e7os e ETLs para Transforma\u00e7\u00e3o de Dados e Processamento de Dados em Escala Processamento de Dados na Nuvem Ciclo de Vida e Automatiza\u00e7\u00e3o de Pipeline de Dados (MLOps) Processamento e Predi\u00e7\u00e3o em Produ\u00e7\u00e3o","title":"Setembro"},{"location":"#outubro","text":"User Interface e Dashboards Data Storytelling","title":"Outubro"},{"location":"#boas-praticas-para-o-modulo","text":"Iremos utilizar fortemente o GitFlow Sugiro que voc\u00eas utilizem o GitGraph para VSCode Sugiro que o VSCode seja nosso ambiente principal de codifica\u00e7\u00e3o Instale o Pyhton 3.12 e sete como global Instale o code tools na op\u00e7\u00e3o de instala\u00e7\u00e3o se for por windows Vamos utilizar muito o Powershell no windows, se acostume! Desinstale o Github Desktop! \u00c9 s\u00e9rio... Tenha instalado o Docker e o Docker Compose Instale o R em sua \u00faltima vers\u00e3o e o RStudio Padronize seu nickname do git no seu computador desta forma: git config --global user.name \"Seu Nome\" git config --global user.email \"seu_email@exemplo.com\" Mantendo o mesmo username e email em todo o m\u00f3dulo! ou instale o gh ou github cli","title":"Boas Pr\u00e1ticas para o M\u00f3dulo"},{"location":"#modulo-11-de-engenharia-de-software-analise-de-dados-com-rstudio","text":"","title":"M\u00f3dulo 11 de Engenharia de Software: An\u00e1lise de Dados com RStudio"},{"location":"#introducao","text":"Nesta aula, vamos explorar a an\u00e1lise de dados avan\u00e7ada utilizando o RStudio e diversas ferramentas de suporte. Nosso parceiro, o Boston Consulting Group (BCG), fornecer\u00e1 arquivos CSV de grande porte para an\u00e1lise. Vamos abordar desde a padroniza\u00e7\u00e3o e normaliza\u00e7\u00e3o dos dados at\u00e9 a constru\u00e7\u00e3o de uma aplica\u00e7\u00e3o de an\u00e1lise de dados utilizando diversas tecnologias. Utilizaremos o conceito de processamento de dados \"Raw\", \"Working\" e \"Trusted\".","title":"Introdu\u00e7\u00e3o"},{"location":"#arquitetura-e-ferramentas","text":"","title":"Arquitetura e Ferramentas"},{"location":"#ferramentas-utilizadas","text":"","title":"Ferramentas Utilizadas"},{"location":"#rstudio","text":"Descri\u00e7\u00e3o : Ambiente de desenvolvimento integrado (IDE) para a linguagem de programa\u00e7\u00e3o R. Uso : An\u00e1lise de dados, gera\u00e7\u00e3o de relat\u00f3rios e visualiza\u00e7\u00f5es. Mais detalhes : RStudio \u00e9 uma IDE poderosa para R que facilita a an\u00e1lise de dados, a cria\u00e7\u00e3o de gr\u00e1ficos e a gera\u00e7\u00e3o de relat\u00f3rios. Ele oferece uma interface amig\u00e1vel e ferramentas integradas para ajudar na codifica\u00e7\u00e3o, depura\u00e7\u00e3o e visualiza\u00e7\u00e3o de dados.","title":"RStudio"},{"location":"#rmarkdown","text":"Descri\u00e7\u00e3o : Ferramenta para criar documentos din\u00e2micos com R. Uso : Gera\u00e7\u00e3o de relat\u00f3rios para cada arquivo CSV analisado. Mais detalhes : RMarkdown permite a cria\u00e7\u00e3o de documentos din\u00e2micos que combinam c\u00f3digo R, texto e visualiza\u00e7\u00f5es. \u00c9 ideal para gerar relat\u00f3rios que podem ser facilmente atualizados com novos dados.","title":"RMarkdown"},{"location":"#appwrite","text":"Descri\u00e7\u00e3o : Plataforma de Backend as a Service (BaaS) que oferece servi\u00e7os como autentica\u00e7\u00e3o, banco de dados, armazenamento e muito mais. Uso : Armazenamento de dados raw e dados processados, utilizando o storage e MariaDB. Mais detalhes : Appwrite \u00e9 uma plataforma BaaS que simplifica o desenvolvimento de aplicativos ao fornecer servi\u00e7os essenciais como autentica\u00e7\u00e3o, banco de dados, armazenamento de arquivos e muito mais, tudo em uma \u00fanica solu\u00e7\u00e3o.","title":"Appwrite"},{"location":"#mariadb","text":"Descri\u00e7\u00e3o : Sistema de gerenciamento de banco de dados relacional. Uso : Armazenamento de dados processados (dados working). Mais detalhes : MariaDB \u00e9 um sistema de gerenciamento de banco de dados relacional que \u00e9 uma bifurca\u00e7\u00e3o do MySQL. Ele \u00e9 conhecido por sua escalabilidade, desempenho e robustez, sendo amplamente utilizado para armazenar dados estruturados.","title":"MariaDB"},{"location":"#elasticsearch","text":"Descri\u00e7\u00e3o : Motor de busca e an\u00e1lise distribu\u00eddo e de c\u00f3digo aberto. Uso : Indexa\u00e7\u00e3o e busca avan\u00e7ada dos dados processados. Mais detalhes : ElasticSearch \u00e9 uma ferramenta poderosa para busca e an\u00e1lise de dados em tempo real. Ele permite a indexa\u00e7\u00e3o r\u00e1pida e eficiente de grandes volumes de dados, facilitando a busca e a an\u00e1lise avan\u00e7ada.","title":"ElasticSearch"},{"location":"#clickhouse","text":"Descri\u00e7\u00e3o : Sistema de gerenciamento de banco de dados orientado a colunas, de c\u00f3digo aberto, para consultas anal\u00edticas em tempo real. Uso : Armazenamento e an\u00e1lise de dados processados (dados trusted). Mais detalhes : ClickHouse \u00e9 um sistema de banco de dados orientado a colunas que \u00e9 otimizado para consultas anal\u00edticas em tempo real. Ele \u00e9 ideal para grandes volumes de dados e oferece desempenho excepcional para an\u00e1lises complexas.","title":"ClickHouse"},{"location":"#streamlit","text":"Descri\u00e7\u00e3o : Framework de c\u00f3digo aberto para a cria\u00e7\u00e3o de aplica\u00e7\u00f5es web interativas em Python. Uso : Constru\u00e7\u00e3o de uma aplica\u00e7\u00e3o de an\u00e1lise de dados (DataApp). Mais detalhes : Streamlit \u00e9 um framework que facilita a cria\u00e7\u00e3o de aplica\u00e7\u00f5es web interativas para an\u00e1lise de dados. Com ele, \u00e9 poss\u00edvel transformar scripts Python em aplicativos web de forma r\u00e1pida e eficiente.","title":"Streamlit"},{"location":"#apache-spark","text":"Descri\u00e7\u00e3o : Motor de an\u00e1lise unificada para processamento de dados em grande escala. Uso : Processamento e predi\u00e7\u00e3o de dados em produ\u00e7\u00e3o. Mais detalhes : Apache Spark \u00e9 uma plataforma de processamento de dados em grande escala que oferece suporte para processamento em lote e em tempo real. Ele \u00e9 amplamente utilizado para an\u00e1lise de big data e aprendizado de m\u00e1quina.","title":"Apache Spark"},{"location":"#docker","text":"Descri\u00e7\u00e3o : Plataforma para desenvolvimento, envio e execu\u00e7\u00e3o de aplica\u00e7\u00f5es em cont\u00eaineres. Uso : Dockeriza\u00e7\u00e3o da aplica\u00e7\u00e3o Streamlit, ElasticSearch e ClickHouse. Mais detalhes : Docker \u00e9 uma plataforma que permite a cria\u00e7\u00e3o, envio e execu\u00e7\u00e3o de aplica\u00e7\u00f5es em cont\u00eaineres. Ele facilita a portabilidade e a escalabilidade das aplica\u00e7\u00f5es, garantindo que elas funcionem de maneira consistente em diferentes ambientes.","title":"Docker"},{"location":"#poetry","text":"Descri\u00e7\u00e3o : Gestor de pacotes e depend\u00eancias para projetos Python. Uso : Gerenciamento de depend\u00eancias da aplica\u00e7\u00e3o Streamlit. Mais detalhes : Poetry \u00e9 uma ferramenta para gerenciamento de depend\u00eancias e pacotes em projetos Python. Ele simplifica a instala\u00e7\u00e3o e atualiza\u00e7\u00e3o de bibliotecas, garantindo que todas as depend\u00eancias sejam resolvidas corretamente.","title":"Poetry"},{"location":"#portainer","text":"Descri\u00e7\u00e3o : Interface de gerenciamento para Docker. Uso : Gerenciamento de cont\u00eaineres Docker. Mais detalhes : Portainer \u00e9 uma interface de usu\u00e1rio para gerenciamento de cont\u00eaineres Docker. Ele facilita a administra\u00e7\u00e3o de ambientes Docker, permitindo a visualiza\u00e7\u00e3o, cria\u00e7\u00e3o e gerenciamento de cont\u00eaineres de forma intuitiva. Descri\u00e7\u00e3o : Interface de gerenciamento para Docker. Uso : Gerenciamento de cont\u00eaineres Docker. Mais detalhes Portainer \u00e9 uma interface de usu\u00e1rio para gerenciamento de cont\u00eaineres Docker. Ele facilita a administra\u00e7\u00e3o de ambientes Docker, permitindo a visualiza\u00e7\u00e3o, cria\u00e7\u00e3o e gerenciamento de cont\u00eaineres de forma intuitiva.","title":"Portainer"},{"location":"#arquitetura-de-processamento-de-dados","text":"Recebimento dos Arquivos CSV - Os arquivos CSV fornecidos pelo BCG ser\u00e3o carregados no RStudio para an\u00e1lise inicial. An\u00e1lise e Padroniza\u00e7\u00e3o dos Dados - Utilizando RStudio, os dados ser\u00e3o ajustados e padronizados. - Relat\u00f3rios ser\u00e3o gerados em RMarkdown para cada arquivo CSV. Armazenamento dos Dados Raw - Os arquivos CSV originais ser\u00e3o armazenados no storage do Appwrite. Normaliza\u00e7\u00e3o e Processamento dos Dados - Os dados ser\u00e3o normalizados e processados no RStudio. - Dados processados (dados working) ser\u00e3o armazenados no MariaDB do Appwrite. Indexa\u00e7\u00e3o e Armazenamento de Dados Trusted - Um cont\u00eainer Docker do ElasticSearch ser\u00e1 configurado para indexa\u00e7\u00e3o e busca avan\u00e7ada. - Alternativamente, um cont\u00eainer Docker do ClickHouse ser\u00e1 configurado para armazenamento e an\u00e1lise de dados processados. - Dados do MariaDB ser\u00e3o indexados no ElasticSearch ou armazenados no ClickHouse. Constru\u00e7\u00e3o da Aplica\u00e7\u00e3o DataApp - Utilizando Streamlit, uma aplica\u00e7\u00e3o interativa ser\u00e1 desenvolvida para consumir dados do MariaDB, ElasticSearch e ClickHouse. - A aplica\u00e7\u00e3o ser\u00e1 dockerizada e gerenciada com Poetry. Processamento com Apache Spark - Apache Spark ser\u00e1 utilizado para realizar predi\u00e7\u00f5es com os dados em produ\u00e7\u00e3o.","title":"Arquitetura de Processamento de Dados"},{"location":"#o-que-e-backend-as-a-service-baas","text":"Backend as a Service (BaaS) \u00e9 um modelo de servi\u00e7o que fornece uma infraestrutura de backend pronta para uso, permitindo que desenvolvedores se concentrem no desenvolvimento do frontend e na l\u00f3gica de neg\u00f3cios. O BaaS oferece servi\u00e7os como autentica\u00e7\u00e3o, banco de dados, armazenamento de arquivos, notifica\u00e7\u00f5es push, entre outros, eliminando a necessidade de gerenciar servidores e infraestrutura.","title":"O que \u00e9 Backend as a Service (BaaS)?"},{"location":"#elasticsearch-vs-clickhouse","text":"","title":"ElasticSearch vs ClickHouse"},{"location":"#elasticsearch_1","text":"Descri\u00e7\u00e3o : Motor de busca e an\u00e1lise distribu\u00eddo, ideal para indexa\u00e7\u00e3o e busca avan\u00e7ada. Vantagens : - Excelente para buscas textuais e an\u00e1lises em tempo real. - Suporte robusto para consultas complexas e agrega\u00e7\u00f5es. Desvantagens : - Pode ser mais complexo de configurar e gerenciar para grandes volumes de dados.","title":"ElasticSearch"},{"location":"#clickhouse_1","text":"Descri\u00e7\u00e3o : Sistema de gerenciamento de banco de dados orientado a colunas, otimizado para consultas anal\u00edticas em tempo real. Vantagens : - Desempenho extremamente r\u00e1pido para consultas anal\u00edticas. - Otimizado para grandes volumes de dados e consultas de leitura intensiva. Desvantagens : - Menos adequado para buscas textuais complexas comparado ao ElasticSearch. Toda a arquitetura ser\u00e1 criada com Docker e rodar\u00e1 on-premise, mas poder\u00e1 ser facilmente integrada em uma nuvem ajustando os ambientes de conex\u00e3o (envs).","title":"ClickHouse"},{"location":"#ferramentas-e-tecnologias","text":"Ferramenta Descri\u00e7\u00e3o Uso RStudio IDE para R An\u00e1lise de dados, gera\u00e7\u00e3o de relat\u00f3rios RMarkdown Ferramenta para documentos din\u00e2micos Gera\u00e7\u00e3o de relat\u00f3rios Appwrite Plataforma de BaaS Armazenamento de dados raw e processados MariaDB Sistema de gerenciamento de banco de dados relacional Armazenamento de dados processados ElasticSearch Motor de busca e an\u00e1lise distribu\u00eddo Indexa\u00e7\u00e3o e busca avan\u00e7ada dos dados ClickHouse Sistema de gerenciamento de banco de dados orientado a colunas Armazenamento e an\u00e1lise de dados processados Streamlit Framework para cria\u00e7\u00e3o de aplica\u00e7\u00f5es web interativas Constru\u00e7\u00e3o de uma aplica\u00e7\u00e3o de an\u00e1lise de dados (DataApp) Apache Spark Motor de an\u00e1lise unificada para processamento de dados em grande escala Processamento e predi\u00e7\u00e3o de dados Docker Plataforma para desenvolvimento, envio e execu\u00e7\u00e3o de aplica\u00e7\u00f5es em cont\u00eaineres Dockeriza\u00e7\u00e3o da aplica\u00e7\u00e3o Streamlit, ElasticSearch e ClickHouse Poetry Gestor de pacotes e depend\u00eancias para projetos Python Gerenciamento de depend\u00eancias da aplica\u00e7\u00e3o Streamlit","title":"Ferramentas e Tecnologias"},{"location":"#mas-isso-nao-e-ferro-e-fogo","text":"Sinta-se a vontade para trazer novas tecnologias para o nosso m\u00f3dulo! Conte com os professores para analisarmos juntos novas solu\u00e7\u00f5es. Aqui vai o refactoring Guru para dar novas ideias \u00e0 voc\u00eas =) Refactoring Guru","title":"Mas isso n\u00e3o \u00e9 ferro e fogo!"},{"location":"arquitetura1/","text":"Arquitetura e Governan\u00e7a de Dados na Nuvem 1 \u00b6 Autoestudos \u00b6 \ud83d\udcda Nome \ud83d\udd17 Link \ud83c\udfa5 O que \u00e9 governan\u00e7a de dados e como funciona? Assistir \ud83d\udcdd Arquiteto de Dados - Por Onde Come\u00e7ar em 5 Passos Ler \ud83c\udfe2 Estruturas Organizacionais Acessar Agenda da Aula \u00b6 Atividade Min Kahoot dos Autoestudos 20 Aula Expositiva de Exemplo de Modelo de Template 40 Vamos subir o Supabase e Analisar uma Arquitetura 60 Total 120 V\u00eddeo da Aula \u00b6 Kahoot \u00b6 \ud83d\udcdfKahoot.it! Template de Arquitetura de Dados na Nuvem \u00b6 Introdu\u00e7\u00e3o \u00b6 Este documento delineia as diretrizes e decis\u00f5es arquiteturais fundamentais para o projeto de Arquitetura e Governan\u00e7a de Dados na Nuvem. Ele fornece uma vis\u00e3o clara da estrutura global, abordando aspectos como padr\u00f5es de design, escolhas tecnol\u00f3gicas, integra\u00e7\u00e3o de sistemas, seguran\u00e7a e escalabilidade. Este template visa garantir consist\u00eancia e coer\u00eancia na implementa\u00e7\u00e3o, facilitando a compreens\u00e3o e promovendo uma abordagem unificada durante o desenvolvimento. Conformidade com Padr\u00f5es Oficiais de Engenharia de Software \u00b6 Este documento baseia-se nos padr\u00f5es oficiais de engenharia de software, incluindo TOGAF e IEEE, adaptando sua estrutura \u00e0s necessidades espec\u00edficas do projeto. Se\u00e7\u00f5es M\u00ednimas Requeridas \u00b6 Requerimentos/User Stories \u00b6 Requerimentos Funcionais \u00b6 RF001 : O sistema deve permitir a inser\u00e7\u00e3o de novos perfis de usu\u00e1rios. RF002 : O sistema deve permitir a atualiza\u00e7\u00e3o de perfis de usu\u00e1rios existentes. RF003 : O sistema deve permitir a exclus\u00e3o de perfis de usu\u00e1rios. RF004 : O sistema deve fornecer uma interface para visualiza\u00e7\u00e3o dos perfis de usu\u00e1rios. Requerimentos N\u00e3o Funcionais \u00b6 RNF001 : O sistema deve ser escal\u00e1vel para suportar um grande n\u00famero de usu\u00e1rios simult\u00e2neos. RNF002 : O sistema deve garantir a seguran\u00e7a dos dados armazenados e transmitidos. RNF003 : O sistema deve ter uma alta disponibilidade, com um tempo de inatividade m\u00e1ximo de 1%. RNF004 : O sistema deve ser us\u00e1vel, com uma interface intuitiva e responsiva. User Stories \u00b6 US001 : Como um administrador, eu quero poder adicionar novos perfis de usu\u00e1rios para gerenciar melhor o acesso ao sistema. US002 : Como um usu\u00e1rio, eu quero atualizar meu perfil para manter minhas informa\u00e7\u00f5es atualizadas. US003 : Como um administrador, eu quero excluir perfis de usu\u00e1rios inativos para manter a base de dados limpa. US004 : Como um usu\u00e1rio, eu quero visualizar meu perfil para verificar minhas informa\u00e7\u00f5es pessoais. Arquitetura (Em UML de Componentes) \u00b6 Vis\u00e3o Geral da Arquitetura \u00b6 Transformar a sua Arquitetura em UML de Componentes \u00b6 A arquitetura do sistema \u00e9 baseada em uma estrutura modular, composta pelos seguintes componentes principais: (Exemplo) Frontend : Interface do usu\u00e1rio desenvolvida em React. Backend : API RESTful desenvolvida em Node.js. Banco de Dados : PostgreSQL, gerenciado pelo Supabase. Autentica\u00e7\u00e3o : Gerenciada pelo Supabase Auth. Armazenamento : Supabase Storage para arquivos est\u00e1ticos. Padr\u00f5es de Design \u00b6 MVC (Model-View-Controller) : Separa\u00e7\u00e3o de responsabilidades entre a interface do usu\u00e1rio, l\u00f3gica de neg\u00f3cios e acesso a dados. RESTful API : Interface de comunica\u00e7\u00e3o entre o frontend e o backend. JWT (JSON Web Tokens) : Para autentica\u00e7\u00e3o e autoriza\u00e7\u00e3o segura. Escolhas Tecnol\u00f3gicas \u00b6 Linguagem de Programa\u00e7\u00e3o : JavaScript/TypeScript Frameworks : React (Frontend), Node.js (Backend) Banco de Dados : PostgreSQL Plataforma de Backend : Supabase Integra\u00e7\u00e3o de Sistemas \u00b6 CI/CD : Integra\u00e7\u00e3o cont\u00ednua e entrega cont\u00ednua usando GitHub Actions. Monitoramento : Ferramentas como Grafana e Prometheus para monitoramento de desempenho e sa\u00fade do sistema. Seguran\u00e7a \u00b6 Autentica\u00e7\u00e3o : Implementa\u00e7\u00e3o de autentica\u00e7\u00e3o baseada em JWT. Autoriza\u00e7\u00e3o : Pol\u00edticas de acesso definidas no Supabase. Criptografia : Dados sens\u00edveis s\u00e3o criptografados em tr\u00e2nsito e em repouso. Escalabilidade \u00b6 Horizontal Scaling : Capacidade de adicionar mais inst\u00e2ncias de servidores conforme a demanda aumenta. Load Balancing : Distribui\u00e7\u00e3o de tr\u00e1fego entre v\u00e1rias inst\u00e2ncias de servidores para otimizar a resposta do sistema. Auditoria \u00b6 Processo de Auditoria \u00b6 Registro de Logs : Todos os eventos importantes s\u00e3o registrados para auditoria. Revis\u00e3o de Seguran\u00e7a : Auditorias regulares de seguran\u00e7a para identificar e corrigir vulnerabilidades. Compliance : Garantia de conformidade com regulamentos como LGPD e GDPR. Efetividade da Navega\u00e7\u00e3o no Documento \u00b6 Este documento foi estruturado para proporcionar uma navega\u00e7\u00e3o eficiente e intuitiva. Utilize o \u00edndice no in\u00edcio do documento para acessar rapidamente as se\u00e7\u00f5es desejadas. Design Visual \u00b6 O design visual deste documento foi pensado para ser consistente e agrad\u00e1vel, melhorando a experi\u00eancia do usu\u00e1rio. Use cabe\u00e7alhos claros, listas e links para facilitar a leitura e a navega\u00e7\u00e3o. TOGAF: The Open Group Architecture Framework \u00b6 TOGAF, ou The Open Group Architecture Framework, \u00e9 um framework de arquitetura empresarial que fornece uma abordagem abrangente para o design, planejamento, implementa\u00e7\u00e3o e governan\u00e7a da arquitetura de uma empresa. Desenvolvido pelo The Open Group, o TOGAF foi lan\u00e7ado pela primeira vez em 1995 e desde ent\u00e3o passou por v\u00e1rias revis\u00f5es, sendo a vers\u00e3o mais recente o TOGAF 9.2, lan\u00e7ado em abril de 2018. O TOGAF \u00e9 composto por v\u00e1rios componentes principais: ADM (Architecture Development Method) : O n\u00facleo do TOGAF, que fornece um processo passo a passo para desenvolver uma arquitetura empresarial. As fases incluem: Preliminary Phase : Prepara\u00e7\u00e3o e inicia\u00e7\u00e3o das atividades de arquitetura. Phase A: Architecture Vision : Defini\u00e7\u00e3o da vis\u00e3o e escopo da arquitetura. Phase B: Business Architecture : Desenvolvimento da arquitetura de neg\u00f3cios. Phase C: Information Systems Architectures : Desenvolvimento das arquiteturas de dados e aplica\u00e7\u00f5es. Phase D: Technology Architecture : Desenvolvimento da arquitetura tecnol\u00f3gica. Phase E: Opportunities and Solutions : Identifica\u00e7\u00e3o de oportunidades e solu\u00e7\u00f5es. Phase F: Migration Planning : Planejamento da migra\u00e7\u00e3o. Phase G: Implementation Governance : Governan\u00e7a da implementa\u00e7\u00e3o. Phase H: Architecture Change Management : Gest\u00e3o de mudan\u00e7as na arquitetura. Requirements Management : Gest\u00e3o cont\u00ednua dos requisitos. Content Framework : Fornece uma estrutura para organizar os artefatos de arquitetura, incluindo modelos, vis\u00f5es e pontos de vista, garantindo que todos os aspectos da arquitetura sejam abordados de forma consistente. Enterprise Continuum : Um reposit\u00f3rio de ativos de arquitetura, incluindo modelos, padr\u00f5es e melhores pr\u00e1ticas, que promove a reutiliza\u00e7\u00e3o e a consist\u00eancia na arquitetura empresarial. TOGAF Reference Models : Inclui v\u00e1rios modelos de refer\u00eancia, como o TRM (Technical Reference Model) e o III-RM (Integrated Information Infrastructure Reference Model), que fornecem orienta\u00e7\u00f5es para o desenvolvimento de arquiteturas espec\u00edficas. Os benef\u00edcios do TOGAF incluem: Alinhamento de TI com Neg\u00f3cios : Garante que a arquitetura de TI esteja alinhada com os objetivos e estrat\u00e9gias de neg\u00f3cios, promovendo uma abordagem integrada e coesa. Redu\u00e7\u00e3o de Custos e Aumento da Efici\u00eancia : Ajuda a reduzir custos, evitar redund\u00e2ncias e aumentar a efici\u00eancia operacional ao fornecer uma abordagem estruturada para o desenvolvimento da arquitetura. Flexibilidade e Adaptabilidade : Pode ser adaptado para atender \u00e0s necessidades espec\u00edficas de diferentes organiza\u00e7\u00f5es e ind\u00fastrias. Melhoria da Governan\u00e7a : Promove a governan\u00e7a eficaz da arquitetura, garantindo que as decis\u00f5es sejam tomadas de forma informada e que as mudan\u00e7as sejam gerenciadas de maneira controlada. Exemplos de aplica\u00e7\u00e3o do TOGAF incluem: Desenvolvimento de uma Arquitetura de Neg\u00f3cios : Utilizando a Phase B do ADM para definir processos de neg\u00f3cios, capacidades e objetivos. Implementa\u00e7\u00e3o de uma Arquitetura de Dados : Utilizando a Phase C do ADM para definir modelos de dados, fluxos de dados e requisitos de armazenamento. Planejamento de Migra\u00e7\u00e3o : Utilizando a Phase F do ADM para criar um plano detalhado de migra\u00e7\u00e3o para uma nova infraestrutura de TI. O The Open Group oferece certifica\u00e7\u00f5es TOGAF para profissionais de arquitetura empresarial, divididas em dois n\u00edveis: TOGAF 9 Foundation (Level 1) : Foco nos conceitos b\u00e1sicos e na terminologia do TOGAF. TOGAF 9 Certified (Level 2) : Foco na aplica\u00e7\u00e3o pr\u00e1tica do TOGAF no desenvolvimento de arquiteturas empresariais. O TOGAF \u00e9 um framework poderoso e amplamente adotado que ajuda as organiza\u00e7\u00f5es a alinhar a TI com os objetivos de neg\u00f3cios, melhorar a efici\u00eancia e gerenciar mudan\u00e7as complexas de maneira eficaz. Para mais informa\u00e7\u00f5es, visite o site oficial do TOGAF . IEEE: Institute of Electrical and Electronics Engineers \u00b6 O IEEE, ou Institute of Electrical and Electronics Engineers, \u00e9 uma organiza\u00e7\u00e3o profissional dedicada ao avan\u00e7o da tecnologia em benef\u00edcio da humanidade. Fundada em 1884, o IEEE \u00e9 a maior organiza\u00e7\u00e3o profissional t\u00e9cnica do mundo, com mais de 400.000 membros em mais de 160 pa\u00edses. O IEEE \u00e9 conhecido por desenvolver padr\u00f5es t\u00e9cnicos que s\u00e3o amplamente adotados em diversas ind\u00fastrias. Esses padr\u00f5es ajudam a garantir a interoperabilidade, seguran\u00e7a e qualidade dos produtos e servi\u00e7os tecnol\u00f3gicos. Principais \u00c1reas de Atua\u00e7\u00e3o \u00b6 Padr\u00f5es T\u00e9cnicos : O IEEE \u00e9 respons\u00e1vel por mais de 1.300 padr\u00f5es ativos, abrangendo uma ampla gama de tecnologias, incluindo: IEEE 802.3 : Padr\u00e3o para Ethernet, que define as caracter\u00edsticas f\u00edsicas e de controle de acesso ao meio para redes locais com fio. IEEE 802.11 : Padr\u00e3o para redes sem fio (Wi-Fi), que define os protocolos para comunica\u00e7\u00e3o sem fio em redes locais. IEEE 1547 : Padr\u00e3o para interconex\u00e3o de recursos de energia distribu\u00edda com sistemas de energia el\u00e9trica. Publica\u00e7\u00f5es e Confer\u00eancias : O IEEE publica cerca de um ter\u00e7o da literatura t\u00e9cnica mundial nas \u00e1reas de engenharia el\u00e9trica, eletr\u00f4nica e ci\u00eancia da computa\u00e7\u00e3o. Al\u00e9m disso, organiza mais de 1.800 confer\u00eancias anuais, proporcionando uma plataforma para a troca de conhecimento e inova\u00e7\u00e3o. Educa\u00e7\u00e3o e Certifica\u00e7\u00e3o : O IEEE oferece uma variedade de programas educacionais e certifica\u00e7\u00f5es para ajudar profissionais a se manterem atualizados com as \u00faltimas inova\u00e7\u00f5es tecnol\u00f3gicas. Exemplos incluem: Certifica\u00e7\u00e3o IEEE Wireless Communication : Foco em tecnologias de comunica\u00e7\u00e3o sem fio. Certifica\u00e7\u00e3o IEEE Software Development : Foco em pr\u00e1ticas e padr\u00f5es de desenvolvimento de software. Comunidades T\u00e9cnicas : O IEEE possui mais de 39 sociedades t\u00e9cnicas e conselhos que cobrem uma ampla gama de disciplinas tecnol\u00f3gicas, permitindo que membros se conectem e colaborem em \u00e1reas espec\u00edficas de interesse. Benef\u00edcios do IEEE \u00b6 Inova\u00e7\u00e3o Tecnol\u00f3gica : O IEEE promove a inova\u00e7\u00e3o tecnol\u00f3gica atrav\u00e9s do desenvolvimento de padr\u00f5es e da dissemina\u00e7\u00e3o de conhecimento t\u00e9cnico. Interoperabilidade : Os padr\u00f5es IEEE garantem que produtos e sistemas de diferentes fabricantes possam trabalhar juntos de forma eficaz e segura. Seguran\u00e7a e Qualidade : Os padr\u00f5es IEEE ajudam a garantir a seguran\u00e7a e a qualidade dos produtos e servi\u00e7os tecnol\u00f3gicos. Desenvolvimento Profissional : O IEEE oferece recursos educacionais e oportunidades de certifica\u00e7\u00e3o para ajudar profissionais a avan\u00e7arem em suas carreiras. Exemplos de aplica\u00e7\u00e3o dos padr\u00f5es IEEE incluem: Redes Locais com Fio : Implementa\u00e7\u00e3o de redes Ethernet baseadas no padr\u00e3o IEEE 802.3, utilizado em escrit\u00f3rios, data centers e ambientes industriais. Redes Sem Fio : Configura\u00e7\u00e3o de redes Wi-Fi baseadas no padr\u00e3o IEEE 802.11, amplamente utilizado em resid\u00eancias, empresas e locais p\u00fablicos. Energia Renov\u00e1vel : Interconex\u00e3o de pain\u00e9is solares e turbinas e\u00f3licas com a rede el\u00e9trica utilizando o padr\u00e3o IEEE 1547. Para mais informa\u00e7\u00f5es, visite o site oficial do IEEE . Pr\u00e1tica (Supabase Self-Hosting com Docker\ud83d\udc0b) \u00b6 Vamos seguir a documenta\u00e7\u00e3o: Link para a Documenta\u00e7\u00e3o Supabase \ud83d\udd6e Vamos explorar o Supabase \ud83e\udde0 \u00b6 \u00c9 neste local que iremos armazenar os dados de processamento Pr\u00e1tica (ClickHouse como DataWareHouse) \u00b6 Link para a Documenta\u00e7\u00e3o ClickHouse \ud83d\udd6e Imagem DockerHub \ud83d\udc0b Vamos instalar o DBeaver\ud83e\udd9b para visualizar os dados: Link de Instala\u00e7\u00e3o Iniciar o ClickHouse via Powershell docker run -d ` --name clickhouse-server ` --ulimit nofile = 262144 :262144 ` -p 8123 :8123 ` -p 9000 :9000 ` -v C: \\p ath \\t o \\c lickhouse \\c onfig:C: \\e tc \\c lickhouse-server \\c onfig.xml ` -v C: \\p ath \\t o \\c lickhouse \\d ata:C: \\v ar \\l ib \\c lickhouse ` yandex/clickhouse-server Iniciar via Linux e OSX docker run -d ` --name clickhouse-server ` --ulimit nofile = 262144 :262144 ` -p 8123 :8123 ` -p 9000 :9000 ` yandex/clickhouse-server Teste de Compara\u00e7\u00e3o \u00b6 1. Cria\u00e7\u00e3o do docker-compose.yml \u00b6 Aqui est\u00e1 o arquivo docker-compose.yml atualizado para incluir a configura\u00e7\u00e3o de senha para o ClickHouse: version : '3.8' services : postgres : image : postgres:latest container_name : postgres environment : POSTGRES_USER : user POSTGRES_PASSWORD : password POSTGRES_DB : testdb ports : - \"5432:5432\" volumes : - pgdata:/var/lib/postgresql/data clickhouse : image : clickhouse/clickhouse-server:latest container_name : clickhouse environment : CLICKHOUSE_USER : user CLICKHOUSE_PASSWORD : password CLICKHOUSE_DB : testdb ports : - \"8123:8123\" - \"9000:9000\" volumes : - clickhouse_data:/var/lib/clickhouse volumes : pgdata : clickhouse_data : 2. Inicializa\u00e7\u00e3o dos Cont\u00eaineres \u00b6 Execute o comando abaixo para iniciar os cont\u00eaineres: docker-compose up -d 3. Cria\u00e7\u00e3o das Tabelas e Inser\u00e7\u00e3o de Dados \u00b6 PostgreSQL \u00b6 Crie um arquivo chamado postgres_setup.sql : -- Cria\u00e7\u00e3o da tabela de teste CREATE TABLE teste ( id SERIAL PRIMARY KEY , nome VARCHAR ( 100 ), valor NUMERIC ); -- Inser\u00e7\u00e3o de dados INSERT INTO teste ( nome , valor ) SELECT 'Nome ' || generate_series ( 1 , 1000000 ), random () * 100 FROM generate_series ( 1 , 1000000 ); Para executar este script dentro do cont\u00eainer PostgreSQL, utilize o comando: docker exec -i postgres psql -U user -d testdb -f /path/to/postgres_setup.sql ClickHouse \u00b6 Crie um arquivo chamado clickhouse_setup.sql : -- Cria\u00e7\u00e3o da tabela de teste CREATE TABLE teste ( id UInt32 , nome String , valor Float32 ) ENGINE = MergeTree () ORDER BY id ; -- Inser\u00e7\u00e3o de dados INSERT INTO teste SELECT number AS id , concat ( 'Nome ' , toString ( number )) AS nome , rand () % 100 AS valor FROM numbers ( 1000000 ); Para executar este script dentro do cont\u00eainer ClickHouse, utilize o comando: docker exec -i clickhouse clickhouse-client --user user --password password --query = \" $( cat /path/to/clickhouse_setup.sql ) \" 4. Execu\u00e7\u00e3o das Consultas de Leitura \u00b6 PostgreSQL \u00b6 \\ timing SELECT * FROM teste WHERE valor > 50 ; Para executar esta consulta dentro do cont\u00eainer PostgreSQL: docker exec -it postgres psql -U user -d testdb -c \"\\timing\" -c \"SELECT * FROM teste WHERE valor > 50;\" ClickHouse \u00b6 SELECT * FROM teste WHERE valor > 50 ; Para executar esta consulta dentro do cont\u00eainer ClickHouse: docker exec -it clickhouse clickhouse-client --user user --password password --query = \"SELECT * FROM teste WHERE valor > 50;\" 5. Limpeza \u00b6 Para remover as tabelas de teste ap\u00f3s a execu\u00e7\u00e3o dos testes: PostgreSQL \u00b6 DROP TABLE teste ; Para executar este comando dentro do cont\u00eainer PostgreSQL: docker exec -it postgres psql -U user -d testdb -c \"DROP TABLE teste;\" ClickHouse \u00b6 DROP TABLE teste ; Para executar este comando dentro do cont\u00eainer ClickHouse: docker exec -it clickhouse clickhouse-client --user user --password password --query = \"DROP TABLE teste;\" Arquitetura e Design \u00b6 Armazenamento em Colunas : ClickHouse : Utiliza uma arquitetura de armazenamento em colunas, o que significa que os dados s\u00e3o armazenados por coluna em vez de por linha. Isso permite uma leitura e compress\u00e3o de dados muito mais eficiente, especialmente para consultas anal\u00edticas que acessam um subconjunto de colunas. PostgreSQL : Utiliza uma arquitetura de armazenamento em linhas, o que \u00e9 mais eficiente para opera\u00e7\u00f5es transacionais (OLTP) onde a maioria das consultas acessa todas as colunas de uma linha espec\u00edfica. Compress\u00e3o de Dados : ClickHouse : A compress\u00e3o de dados em colunas \u00e9 mais eficiente porque dados semelhantes s\u00e3o armazenados juntos, permitindo algoritmos de compress\u00e3o mais eficazes. PostgreSQL : Embora suporte compress\u00e3o, a efici\u00eancia \u00e9 menor devido ao armazenamento em linhas. Processamento em Modo Vetorial : ClickHouse : Utiliza processamento em modo vetorial, onde as opera\u00e7\u00f5es s\u00e3o realizadas em blocos de dados ao inv\u00e9s de tuplas individuais, resultando em melhor utiliza\u00e7\u00e3o da CPU e maior throughput. PostgreSQL : Processa dados em tuplas individuais, o que \u00e9 menos eficiente para grandes volumes de dados. Performance em Consultas \u00b6 Consultas Anal\u00edticas : ClickHouse : Projetado especificamente para consultas anal\u00edticas (OLAP), oferecendo desempenho superior em agrega\u00e7\u00f5es, filtragens e c\u00e1lculos em grandes conjuntos de dados. PostgreSQL : Embora possa executar consultas anal\u00edticas, n\u00e3o \u00e9 otimizado para tal e pode ser significativamente mais lento em compara\u00e7\u00e3o com ClickHouse. Indexa\u00e7\u00e3o : ClickHouse : Utiliza \u00edndices prim\u00e1rios baseados em ordena\u00e7\u00e3o e \u00edndices de dados em colunas, otimizados para leitura r\u00e1pida. PostgreSQL : Utiliza \u00edndices B-tree e outros tipos de \u00edndices que s\u00e3o mais vers\u00e1teis, mas podem ser menos eficientes para consultas anal\u00edticas em grandes volumes de dados. Trade-offs \u00b6 Transa\u00e7\u00f5es e Consist\u00eancia : ClickHouse : N\u00e3o oferece suporte completo a transa\u00e7\u00f5es ACID (Atomicidade, Consist\u00eancia, Isolamento e Durabilidade). \u00c9 otimizado para leituras r\u00e1pidas e escritas em massa, mas pode n\u00e3o garantir a consist\u00eancia em todas as opera\u00e7\u00f5es. PostgreSQL : Oferece suporte completo a transa\u00e7\u00f5es ACID, sendo ideal para aplica\u00e7\u00f5es OLTP onde a consist\u00eancia e a integridade dos dados s\u00e3o cruciais. Flexibilidade e Funcionalidades : ClickHouse : Focado em consultas anal\u00edticas, pode n\u00e3o oferecer todas as funcionalidades avan\u00e7adas de um banco de dados relacional completo, como suporte a joins complexos, triggers e stored procedures. PostgreSQL : Extremamente vers\u00e1til, oferece uma ampla gama de funcionalidades, incluindo suporte a joins complexos, triggers, stored procedures, e extens\u00f5es como PostGIS para dados geoespaciais. Escritas e Atualiza\u00e7\u00f5es : ClickHouse : Otimizado para leituras r\u00e1pidas, mas as opera\u00e7\u00f5es de escrita e atualiza\u00e7\u00e3o podem ser menos eficientes. N\u00e3o \u00e9 ideal para cargas de trabalho que envolvem muitas atualiza\u00e7\u00f5es frequentes de dados. PostgreSQL : Melhor equilibrado para opera\u00e7\u00f5es de leitura e escrita, sendo adequado tanto para OLTP quanto para OLAP, embora n\u00e3o seja t\u00e3o r\u00e1pido quanto ClickHouse para grandes consultas anal\u00edticas. Ecossistema e Suporte : ClickHouse : Comunidade em crescimento, mas ainda mais recente em compara\u00e7\u00e3o com PostgreSQL. Pode haver menos ferramentas e bibliotecas dispon\u00edveis. PostgreSQL : Ecossistema maduro com uma vasta gama de ferramentas, bibliotecas e suporte comunit\u00e1rio e comercial. Introdu\u00e7\u00e3o ao Parquet \u00b6 Parquet \u00e9 um formato de armazenamento de dados em colunas desenvolvido para o ecossistema Hadoop. \u00c9 projetado para ser altamente eficiente tanto em termos de espa\u00e7o quanto de desempenho, especialmente para grandes volumes de dados. Ele \u00e9 amplamente utilizado em ambientes de Big Data devido \u00e0 sua capacidade de suportar leitura e escrita eficientes. Introdu\u00e7\u00e3o ao CSV \u00b6 CSV (Comma-Separated Values) \u00e9 um formato de arquivo simples e amplamente utilizado para armazenar dados tabulares em texto plano. Cada linha do arquivo representa um registro e cada campo \u00e9 separado por uma v\u00edrgula (ou outro delimitador, como ponto e v\u00edrgula). Compara\u00e7\u00e3o: Parquet vs. CSV \u00b6 Estrutura de Armazenamento \u00b6 Parquet : Armazenamento em Colunas : Os dados s\u00e3o armazenados por coluna, o que permite uma leitura e compress\u00e3o mais eficiente, especialmente para consultas que acessam um subconjunto de colunas. Metadados : Inclui metadados ricos que descrevem o esquema dos dados, tipos de dados e outras informa\u00e7\u00f5es \u00fateis. CSV : Armazenamento em Linhas : Os dados s\u00e3o armazenados por linha, o que \u00e9 simples e direto, mas menos eficiente para grandes volumes de dados e consultas que acessam apenas algumas colunas. Metadados : N\u00e3o possui metadados embutidos. O esquema dos dados deve ser inferido ou fornecido separadamente. Desempenho \u00b6 Parquet : Leitura e Escrita : Muito eficiente para leitura de grandes volumes de dados, especialmente quando apenas algumas colunas s\u00e3o necess\u00e1rias. A escrita pode ser mais lenta devido \u00e0 compress\u00e3o e ao armazenamento em colunas. Compress\u00e3o : Suporta compress\u00e3o eficiente de dados, reduzindo significativamente o espa\u00e7o de armazenamento necess\u00e1rio. Consultas Anal\u00edticas : Excelente desempenho para consultas anal\u00edticas devido ao armazenamento em colunas. CSV : Leitura e Escrita : Leitura e escrita simples e r\u00e1pidas, mas pode ser ineficiente para grandes volumes de dados. Compress\u00e3o : N\u00e3o suporta compress\u00e3o nativamente. Pode ser comprimido usando ferramentas externas (por exemplo, gzip), mas isso n\u00e3o \u00e9 t\u00e3o eficiente quanto a compress\u00e3o nativa do Parquet. Consultas Anal\u00edticas : Desempenho inferior para consultas anal\u00edticas em grandes volumes de dados devido ao armazenamento em linhas. Flexibilidade e Usabilidade \u00b6 Parquet : Complexidade : Mais complexo de usar e configurar devido \u00e0 necessidade de bibliotecas espec\u00edficas para leitura e escrita. Compatibilidade : Amplamente suportado em ferramentas de Big Data, como Apache Spark, Apache Hive, e Apache Drill. CSV : Simplicidade : Muito simples de usar e entender. Pode ser aberto e editado com qualquer editor de texto ou planilha. Compatibilidade : Suportado por praticamente todas as ferramentas de software, desde simples editores de texto at\u00e9 sistemas de banco de dados e ferramentas de an\u00e1lise de dados. Casos de Uso \u00b6 Parquet : Ideal para grandes volumes de dados e cargas de trabalho anal\u00edticas. Usado em ambientes de Big Data onde a efici\u00eancia de armazenamento e desempenho de leitura s\u00e3o cr\u00edticos. CSV : Adequado para pequenos a m\u00e9dios volumes de dados. Ideal para interc\u00e2mbio de dados entre sistemas e para uso em ferramentas que n\u00e3o suportam formatos de dados mais complexos. Para o Nosso Data Lake em Storages do Supabase: \u00b6 install.packages ( \"arrow\" ) library ( arrow ) # Caminho para o arquivo CSV csv_file <- \"caminho/para/seu/arquivo.csv\" # Caminho para o arquivo Parquet de sa\u00edda parquet_file <- \"caminho/para/seu/arquivo.parquet\" # Ler o arquivo CSV data <- read.csv ( csv_file ) # Escrever o arquivo Parquet write_parquet ( data , parquet_file ) cat ( \"Arquivo CSV transformado em Parquet com sucesso!\\n\" ) pip install pandas pyarrow import pandas as pd # Caminho para o arquivo CSV csv_file = \"caminho/para/seu/arquivo.csv\" # Caminho para o arquivo Parquet de sa\u00edda parquet_file = \"caminho/para/seu/arquivo.parquet\" # Ler o arquivo CSV df = pd . read_csv ( csv_file ) # Escrever o arquivo Parquet df . to_parquet ( parquet_file , engine = 'pyarrow' ) print ( \"Arquivo CSV transformado em Parquet com sucesso!\" ) Exemplo de Arquitetura Visual -> Grupo 01 do Curso de SI do Inteli em 2023.4 \u00b6","title":"Arquitetura e Governan\u00e7a de Dados na Nuvem 1"},{"location":"arquitetura1/#arquitetura-e-governanca-de-dados-na-nuvem-1","text":"","title":"Arquitetura e Governan\u00e7a de Dados na Nuvem 1"},{"location":"arquitetura1/#autoestudos","text":"\ud83d\udcda Nome \ud83d\udd17 Link \ud83c\udfa5 O que \u00e9 governan\u00e7a de dados e como funciona? Assistir \ud83d\udcdd Arquiteto de Dados - Por Onde Come\u00e7ar em 5 Passos Ler \ud83c\udfe2 Estruturas Organizacionais Acessar","title":"Autoestudos"},{"location":"arquitetura1/#agenda-da-aula","text":"Atividade Min Kahoot dos Autoestudos 20 Aula Expositiva de Exemplo de Modelo de Template 40 Vamos subir o Supabase e Analisar uma Arquitetura 60 Total 120","title":"Agenda da Aula"},{"location":"arquitetura1/#video-da-aula","text":"","title":"V\u00eddeo da Aula"},{"location":"arquitetura1/#kahoot","text":"\ud83d\udcdfKahoot.it!","title":"Kahoot"},{"location":"arquitetura1/#template-de-arquitetura-de-dados-na-nuvem","text":"","title":"Template de Arquitetura de Dados na Nuvem"},{"location":"arquitetura1/#introducao","text":"Este documento delineia as diretrizes e decis\u00f5es arquiteturais fundamentais para o projeto de Arquitetura e Governan\u00e7a de Dados na Nuvem. Ele fornece uma vis\u00e3o clara da estrutura global, abordando aspectos como padr\u00f5es de design, escolhas tecnol\u00f3gicas, integra\u00e7\u00e3o de sistemas, seguran\u00e7a e escalabilidade. Este template visa garantir consist\u00eancia e coer\u00eancia na implementa\u00e7\u00e3o, facilitando a compreens\u00e3o e promovendo uma abordagem unificada durante o desenvolvimento.","title":"Introdu\u00e7\u00e3o"},{"location":"arquitetura1/#conformidade-com-padroes-oficiais-de-engenharia-de-software","text":"Este documento baseia-se nos padr\u00f5es oficiais de engenharia de software, incluindo TOGAF e IEEE, adaptando sua estrutura \u00e0s necessidades espec\u00edficas do projeto.","title":"Conformidade com Padr\u00f5es Oficiais de Engenharia de Software"},{"location":"arquitetura1/#secoes-minimas-requeridas","text":"","title":"Se\u00e7\u00f5es M\u00ednimas Requeridas"},{"location":"arquitetura1/#requerimentosuser-stories","text":"","title":"Requerimentos/User Stories"},{"location":"arquitetura1/#requerimentos-funcionais","text":"RF001 : O sistema deve permitir a inser\u00e7\u00e3o de novos perfis de usu\u00e1rios. RF002 : O sistema deve permitir a atualiza\u00e7\u00e3o de perfis de usu\u00e1rios existentes. RF003 : O sistema deve permitir a exclus\u00e3o de perfis de usu\u00e1rios. RF004 : O sistema deve fornecer uma interface para visualiza\u00e7\u00e3o dos perfis de usu\u00e1rios.","title":"Requerimentos Funcionais"},{"location":"arquitetura1/#requerimentos-nao-funcionais","text":"RNF001 : O sistema deve ser escal\u00e1vel para suportar um grande n\u00famero de usu\u00e1rios simult\u00e2neos. RNF002 : O sistema deve garantir a seguran\u00e7a dos dados armazenados e transmitidos. RNF003 : O sistema deve ter uma alta disponibilidade, com um tempo de inatividade m\u00e1ximo de 1%. RNF004 : O sistema deve ser us\u00e1vel, com uma interface intuitiva e responsiva.","title":"Requerimentos N\u00e3o Funcionais"},{"location":"arquitetura1/#user-stories","text":"US001 : Como um administrador, eu quero poder adicionar novos perfis de usu\u00e1rios para gerenciar melhor o acesso ao sistema. US002 : Como um usu\u00e1rio, eu quero atualizar meu perfil para manter minhas informa\u00e7\u00f5es atualizadas. US003 : Como um administrador, eu quero excluir perfis de usu\u00e1rios inativos para manter a base de dados limpa. US004 : Como um usu\u00e1rio, eu quero visualizar meu perfil para verificar minhas informa\u00e7\u00f5es pessoais.","title":"User Stories"},{"location":"arquitetura1/#arquitetura-em-uml-de-componentes","text":"","title":"Arquitetura (Em UML de Componentes)"},{"location":"arquitetura1/#visao-geral-da-arquitetura","text":"","title":"Vis\u00e3o Geral da Arquitetura"},{"location":"arquitetura1/#transformar-a-sua-arquitetura-em-uml-de-componentes","text":"A arquitetura do sistema \u00e9 baseada em uma estrutura modular, composta pelos seguintes componentes principais: (Exemplo) Frontend : Interface do usu\u00e1rio desenvolvida em React. Backend : API RESTful desenvolvida em Node.js. Banco de Dados : PostgreSQL, gerenciado pelo Supabase. Autentica\u00e7\u00e3o : Gerenciada pelo Supabase Auth. Armazenamento : Supabase Storage para arquivos est\u00e1ticos.","title":"Transformar a sua Arquitetura em UML de Componentes"},{"location":"arquitetura1/#padroes-de-design","text":"MVC (Model-View-Controller) : Separa\u00e7\u00e3o de responsabilidades entre a interface do usu\u00e1rio, l\u00f3gica de neg\u00f3cios e acesso a dados. RESTful API : Interface de comunica\u00e7\u00e3o entre o frontend e o backend. JWT (JSON Web Tokens) : Para autentica\u00e7\u00e3o e autoriza\u00e7\u00e3o segura.","title":"Padr\u00f5es de Design"},{"location":"arquitetura1/#escolhas-tecnologicas","text":"Linguagem de Programa\u00e7\u00e3o : JavaScript/TypeScript Frameworks : React (Frontend), Node.js (Backend) Banco de Dados : PostgreSQL Plataforma de Backend : Supabase","title":"Escolhas Tecnol\u00f3gicas"},{"location":"arquitetura1/#integracao-de-sistemas","text":"CI/CD : Integra\u00e7\u00e3o cont\u00ednua e entrega cont\u00ednua usando GitHub Actions. Monitoramento : Ferramentas como Grafana e Prometheus para monitoramento de desempenho e sa\u00fade do sistema.","title":"Integra\u00e7\u00e3o de Sistemas"},{"location":"arquitetura1/#seguranca","text":"Autentica\u00e7\u00e3o : Implementa\u00e7\u00e3o de autentica\u00e7\u00e3o baseada em JWT. Autoriza\u00e7\u00e3o : Pol\u00edticas de acesso definidas no Supabase. Criptografia : Dados sens\u00edveis s\u00e3o criptografados em tr\u00e2nsito e em repouso.","title":"Seguran\u00e7a"},{"location":"arquitetura1/#escalabilidade","text":"Horizontal Scaling : Capacidade de adicionar mais inst\u00e2ncias de servidores conforme a demanda aumenta. Load Balancing : Distribui\u00e7\u00e3o de tr\u00e1fego entre v\u00e1rias inst\u00e2ncias de servidores para otimizar a resposta do sistema.","title":"Escalabilidade"},{"location":"arquitetura1/#auditoria","text":"","title":"Auditoria"},{"location":"arquitetura1/#processo-de-auditoria","text":"Registro de Logs : Todos os eventos importantes s\u00e3o registrados para auditoria. Revis\u00e3o de Seguran\u00e7a : Auditorias regulares de seguran\u00e7a para identificar e corrigir vulnerabilidades. Compliance : Garantia de conformidade com regulamentos como LGPD e GDPR.","title":"Processo de Auditoria"},{"location":"arquitetura1/#efetividade-da-navegacao-no-documento","text":"Este documento foi estruturado para proporcionar uma navega\u00e7\u00e3o eficiente e intuitiva. Utilize o \u00edndice no in\u00edcio do documento para acessar rapidamente as se\u00e7\u00f5es desejadas.","title":"Efetividade da Navega\u00e7\u00e3o no Documento"},{"location":"arquitetura1/#design-visual","text":"O design visual deste documento foi pensado para ser consistente e agrad\u00e1vel, melhorando a experi\u00eancia do usu\u00e1rio. Use cabe\u00e7alhos claros, listas e links para facilitar a leitura e a navega\u00e7\u00e3o.","title":"Design Visual"},{"location":"arquitetura1/#togaf-the-open-group-architecture-framework","text":"TOGAF, ou The Open Group Architecture Framework, \u00e9 um framework de arquitetura empresarial que fornece uma abordagem abrangente para o design, planejamento, implementa\u00e7\u00e3o e governan\u00e7a da arquitetura de uma empresa. Desenvolvido pelo The Open Group, o TOGAF foi lan\u00e7ado pela primeira vez em 1995 e desde ent\u00e3o passou por v\u00e1rias revis\u00f5es, sendo a vers\u00e3o mais recente o TOGAF 9.2, lan\u00e7ado em abril de 2018. O TOGAF \u00e9 composto por v\u00e1rios componentes principais: ADM (Architecture Development Method) : O n\u00facleo do TOGAF, que fornece um processo passo a passo para desenvolver uma arquitetura empresarial. As fases incluem: Preliminary Phase : Prepara\u00e7\u00e3o e inicia\u00e7\u00e3o das atividades de arquitetura. Phase A: Architecture Vision : Defini\u00e7\u00e3o da vis\u00e3o e escopo da arquitetura. Phase B: Business Architecture : Desenvolvimento da arquitetura de neg\u00f3cios. Phase C: Information Systems Architectures : Desenvolvimento das arquiteturas de dados e aplica\u00e7\u00f5es. Phase D: Technology Architecture : Desenvolvimento da arquitetura tecnol\u00f3gica. Phase E: Opportunities and Solutions : Identifica\u00e7\u00e3o de oportunidades e solu\u00e7\u00f5es. Phase F: Migration Planning : Planejamento da migra\u00e7\u00e3o. Phase G: Implementation Governance : Governan\u00e7a da implementa\u00e7\u00e3o. Phase H: Architecture Change Management : Gest\u00e3o de mudan\u00e7as na arquitetura. Requirements Management : Gest\u00e3o cont\u00ednua dos requisitos. Content Framework : Fornece uma estrutura para organizar os artefatos de arquitetura, incluindo modelos, vis\u00f5es e pontos de vista, garantindo que todos os aspectos da arquitetura sejam abordados de forma consistente. Enterprise Continuum : Um reposit\u00f3rio de ativos de arquitetura, incluindo modelos, padr\u00f5es e melhores pr\u00e1ticas, que promove a reutiliza\u00e7\u00e3o e a consist\u00eancia na arquitetura empresarial. TOGAF Reference Models : Inclui v\u00e1rios modelos de refer\u00eancia, como o TRM (Technical Reference Model) e o III-RM (Integrated Information Infrastructure Reference Model), que fornecem orienta\u00e7\u00f5es para o desenvolvimento de arquiteturas espec\u00edficas. Os benef\u00edcios do TOGAF incluem: Alinhamento de TI com Neg\u00f3cios : Garante que a arquitetura de TI esteja alinhada com os objetivos e estrat\u00e9gias de neg\u00f3cios, promovendo uma abordagem integrada e coesa. Redu\u00e7\u00e3o de Custos e Aumento da Efici\u00eancia : Ajuda a reduzir custos, evitar redund\u00e2ncias e aumentar a efici\u00eancia operacional ao fornecer uma abordagem estruturada para o desenvolvimento da arquitetura. Flexibilidade e Adaptabilidade : Pode ser adaptado para atender \u00e0s necessidades espec\u00edficas de diferentes organiza\u00e7\u00f5es e ind\u00fastrias. Melhoria da Governan\u00e7a : Promove a governan\u00e7a eficaz da arquitetura, garantindo que as decis\u00f5es sejam tomadas de forma informada e que as mudan\u00e7as sejam gerenciadas de maneira controlada. Exemplos de aplica\u00e7\u00e3o do TOGAF incluem: Desenvolvimento de uma Arquitetura de Neg\u00f3cios : Utilizando a Phase B do ADM para definir processos de neg\u00f3cios, capacidades e objetivos. Implementa\u00e7\u00e3o de uma Arquitetura de Dados : Utilizando a Phase C do ADM para definir modelos de dados, fluxos de dados e requisitos de armazenamento. Planejamento de Migra\u00e7\u00e3o : Utilizando a Phase F do ADM para criar um plano detalhado de migra\u00e7\u00e3o para uma nova infraestrutura de TI. O The Open Group oferece certifica\u00e7\u00f5es TOGAF para profissionais de arquitetura empresarial, divididas em dois n\u00edveis: TOGAF 9 Foundation (Level 1) : Foco nos conceitos b\u00e1sicos e na terminologia do TOGAF. TOGAF 9 Certified (Level 2) : Foco na aplica\u00e7\u00e3o pr\u00e1tica do TOGAF no desenvolvimento de arquiteturas empresariais. O TOGAF \u00e9 um framework poderoso e amplamente adotado que ajuda as organiza\u00e7\u00f5es a alinhar a TI com os objetivos de neg\u00f3cios, melhorar a efici\u00eancia e gerenciar mudan\u00e7as complexas de maneira eficaz. Para mais informa\u00e7\u00f5es, visite o site oficial do TOGAF .","title":"TOGAF: The Open Group Architecture Framework"},{"location":"arquitetura1/#ieee-institute-of-electrical-and-electronics-engineers","text":"O IEEE, ou Institute of Electrical and Electronics Engineers, \u00e9 uma organiza\u00e7\u00e3o profissional dedicada ao avan\u00e7o da tecnologia em benef\u00edcio da humanidade. Fundada em 1884, o IEEE \u00e9 a maior organiza\u00e7\u00e3o profissional t\u00e9cnica do mundo, com mais de 400.000 membros em mais de 160 pa\u00edses. O IEEE \u00e9 conhecido por desenvolver padr\u00f5es t\u00e9cnicos que s\u00e3o amplamente adotados em diversas ind\u00fastrias. Esses padr\u00f5es ajudam a garantir a interoperabilidade, seguran\u00e7a e qualidade dos produtos e servi\u00e7os tecnol\u00f3gicos.","title":"IEEE: Institute of Electrical and Electronics Engineers"},{"location":"arquitetura1/#principais-areas-de-atuacao","text":"Padr\u00f5es T\u00e9cnicos : O IEEE \u00e9 respons\u00e1vel por mais de 1.300 padr\u00f5es ativos, abrangendo uma ampla gama de tecnologias, incluindo: IEEE 802.3 : Padr\u00e3o para Ethernet, que define as caracter\u00edsticas f\u00edsicas e de controle de acesso ao meio para redes locais com fio. IEEE 802.11 : Padr\u00e3o para redes sem fio (Wi-Fi), que define os protocolos para comunica\u00e7\u00e3o sem fio em redes locais. IEEE 1547 : Padr\u00e3o para interconex\u00e3o de recursos de energia distribu\u00edda com sistemas de energia el\u00e9trica. Publica\u00e7\u00f5es e Confer\u00eancias : O IEEE publica cerca de um ter\u00e7o da literatura t\u00e9cnica mundial nas \u00e1reas de engenharia el\u00e9trica, eletr\u00f4nica e ci\u00eancia da computa\u00e7\u00e3o. Al\u00e9m disso, organiza mais de 1.800 confer\u00eancias anuais, proporcionando uma plataforma para a troca de conhecimento e inova\u00e7\u00e3o. Educa\u00e7\u00e3o e Certifica\u00e7\u00e3o : O IEEE oferece uma variedade de programas educacionais e certifica\u00e7\u00f5es para ajudar profissionais a se manterem atualizados com as \u00faltimas inova\u00e7\u00f5es tecnol\u00f3gicas. Exemplos incluem: Certifica\u00e7\u00e3o IEEE Wireless Communication : Foco em tecnologias de comunica\u00e7\u00e3o sem fio. Certifica\u00e7\u00e3o IEEE Software Development : Foco em pr\u00e1ticas e padr\u00f5es de desenvolvimento de software. Comunidades T\u00e9cnicas : O IEEE possui mais de 39 sociedades t\u00e9cnicas e conselhos que cobrem uma ampla gama de disciplinas tecnol\u00f3gicas, permitindo que membros se conectem e colaborem em \u00e1reas espec\u00edficas de interesse.","title":"Principais \u00c1reas de Atua\u00e7\u00e3o"},{"location":"arquitetura1/#beneficios-do-ieee","text":"Inova\u00e7\u00e3o Tecnol\u00f3gica : O IEEE promove a inova\u00e7\u00e3o tecnol\u00f3gica atrav\u00e9s do desenvolvimento de padr\u00f5es e da dissemina\u00e7\u00e3o de conhecimento t\u00e9cnico. Interoperabilidade : Os padr\u00f5es IEEE garantem que produtos e sistemas de diferentes fabricantes possam trabalhar juntos de forma eficaz e segura. Seguran\u00e7a e Qualidade : Os padr\u00f5es IEEE ajudam a garantir a seguran\u00e7a e a qualidade dos produtos e servi\u00e7os tecnol\u00f3gicos. Desenvolvimento Profissional : O IEEE oferece recursos educacionais e oportunidades de certifica\u00e7\u00e3o para ajudar profissionais a avan\u00e7arem em suas carreiras. Exemplos de aplica\u00e7\u00e3o dos padr\u00f5es IEEE incluem: Redes Locais com Fio : Implementa\u00e7\u00e3o de redes Ethernet baseadas no padr\u00e3o IEEE 802.3, utilizado em escrit\u00f3rios, data centers e ambientes industriais. Redes Sem Fio : Configura\u00e7\u00e3o de redes Wi-Fi baseadas no padr\u00e3o IEEE 802.11, amplamente utilizado em resid\u00eancias, empresas e locais p\u00fablicos. Energia Renov\u00e1vel : Interconex\u00e3o de pain\u00e9is solares e turbinas e\u00f3licas com a rede el\u00e9trica utilizando o padr\u00e3o IEEE 1547. Para mais informa\u00e7\u00f5es, visite o site oficial do IEEE .","title":"Benef\u00edcios do IEEE"},{"location":"arquitetura1/#pratica-supabase-self-hosting-com-docker","text":"Vamos seguir a documenta\u00e7\u00e3o: Link para a Documenta\u00e7\u00e3o Supabase \ud83d\udd6e","title":"Pr\u00e1tica (Supabase Self-Hosting com Docker\ud83d\udc0b)"},{"location":"arquitetura1/#vamos-explorar-o-supabase","text":"\u00c9 neste local que iremos armazenar os dados de processamento","title":"Vamos explorar o Supabase \ud83e\udde0"},{"location":"arquitetura1/#pratica-clickhouse-como-datawarehouse","text":"Link para a Documenta\u00e7\u00e3o ClickHouse \ud83d\udd6e Imagem DockerHub \ud83d\udc0b Vamos instalar o DBeaver\ud83e\udd9b para visualizar os dados: Link de Instala\u00e7\u00e3o Iniciar o ClickHouse via Powershell docker run -d ` --name clickhouse-server ` --ulimit nofile = 262144 :262144 ` -p 8123 :8123 ` -p 9000 :9000 ` -v C: \\p ath \\t o \\c lickhouse \\c onfig:C: \\e tc \\c lickhouse-server \\c onfig.xml ` -v C: \\p ath \\t o \\c lickhouse \\d ata:C: \\v ar \\l ib \\c lickhouse ` yandex/clickhouse-server Iniciar via Linux e OSX docker run -d ` --name clickhouse-server ` --ulimit nofile = 262144 :262144 ` -p 8123 :8123 ` -p 9000 :9000 ` yandex/clickhouse-server","title":"Pr\u00e1tica (ClickHouse como DataWareHouse)"},{"location":"arquitetura1/#teste-de-comparacao","text":"","title":"Teste de Compara\u00e7\u00e3o"},{"location":"arquitetura1/#1-criacao-do-docker-composeyml","text":"Aqui est\u00e1 o arquivo docker-compose.yml atualizado para incluir a configura\u00e7\u00e3o de senha para o ClickHouse: version : '3.8' services : postgres : image : postgres:latest container_name : postgres environment : POSTGRES_USER : user POSTGRES_PASSWORD : password POSTGRES_DB : testdb ports : - \"5432:5432\" volumes : - pgdata:/var/lib/postgresql/data clickhouse : image : clickhouse/clickhouse-server:latest container_name : clickhouse environment : CLICKHOUSE_USER : user CLICKHOUSE_PASSWORD : password CLICKHOUSE_DB : testdb ports : - \"8123:8123\" - \"9000:9000\" volumes : - clickhouse_data:/var/lib/clickhouse volumes : pgdata : clickhouse_data :","title":"1. Cria\u00e7\u00e3o do docker-compose.yml"},{"location":"arquitetura1/#2-inicializacao-dos-conteineres","text":"Execute o comando abaixo para iniciar os cont\u00eaineres: docker-compose up -d","title":"2. Inicializa\u00e7\u00e3o dos Cont\u00eaineres"},{"location":"arquitetura1/#3-criacao-das-tabelas-e-insercao-de-dados","text":"","title":"3. Cria\u00e7\u00e3o das Tabelas e Inser\u00e7\u00e3o de Dados"},{"location":"arquitetura1/#postgresql","text":"Crie um arquivo chamado postgres_setup.sql : -- Cria\u00e7\u00e3o da tabela de teste CREATE TABLE teste ( id SERIAL PRIMARY KEY , nome VARCHAR ( 100 ), valor NUMERIC ); -- Inser\u00e7\u00e3o de dados INSERT INTO teste ( nome , valor ) SELECT 'Nome ' || generate_series ( 1 , 1000000 ), random () * 100 FROM generate_series ( 1 , 1000000 ); Para executar este script dentro do cont\u00eainer PostgreSQL, utilize o comando: docker exec -i postgres psql -U user -d testdb -f /path/to/postgres_setup.sql","title":"PostgreSQL"},{"location":"arquitetura1/#clickhouse","text":"Crie um arquivo chamado clickhouse_setup.sql : -- Cria\u00e7\u00e3o da tabela de teste CREATE TABLE teste ( id UInt32 , nome String , valor Float32 ) ENGINE = MergeTree () ORDER BY id ; -- Inser\u00e7\u00e3o de dados INSERT INTO teste SELECT number AS id , concat ( 'Nome ' , toString ( number )) AS nome , rand () % 100 AS valor FROM numbers ( 1000000 ); Para executar este script dentro do cont\u00eainer ClickHouse, utilize o comando: docker exec -i clickhouse clickhouse-client --user user --password password --query = \" $( cat /path/to/clickhouse_setup.sql ) \"","title":"ClickHouse"},{"location":"arquitetura1/#4-execucao-das-consultas-de-leitura","text":"","title":"4. Execu\u00e7\u00e3o das Consultas de Leitura"},{"location":"arquitetura1/#postgresql_1","text":"\\ timing SELECT * FROM teste WHERE valor > 50 ; Para executar esta consulta dentro do cont\u00eainer PostgreSQL: docker exec -it postgres psql -U user -d testdb -c \"\\timing\" -c \"SELECT * FROM teste WHERE valor > 50;\"","title":"PostgreSQL"},{"location":"arquitetura1/#clickhouse_1","text":"SELECT * FROM teste WHERE valor > 50 ; Para executar esta consulta dentro do cont\u00eainer ClickHouse: docker exec -it clickhouse clickhouse-client --user user --password password --query = \"SELECT * FROM teste WHERE valor > 50;\"","title":"ClickHouse"},{"location":"arquitetura1/#5-limpeza","text":"Para remover as tabelas de teste ap\u00f3s a execu\u00e7\u00e3o dos testes:","title":"5. Limpeza"},{"location":"arquitetura1/#postgresql_2","text":"DROP TABLE teste ; Para executar este comando dentro do cont\u00eainer PostgreSQL: docker exec -it postgres psql -U user -d testdb -c \"DROP TABLE teste;\"","title":"PostgreSQL"},{"location":"arquitetura1/#clickhouse_2","text":"DROP TABLE teste ; Para executar este comando dentro do cont\u00eainer ClickHouse: docker exec -it clickhouse clickhouse-client --user user --password password --query = \"DROP TABLE teste;\"","title":"ClickHouse"},{"location":"arquitetura1/#arquitetura-e-design","text":"Armazenamento em Colunas : ClickHouse : Utiliza uma arquitetura de armazenamento em colunas, o que significa que os dados s\u00e3o armazenados por coluna em vez de por linha. Isso permite uma leitura e compress\u00e3o de dados muito mais eficiente, especialmente para consultas anal\u00edticas que acessam um subconjunto de colunas. PostgreSQL : Utiliza uma arquitetura de armazenamento em linhas, o que \u00e9 mais eficiente para opera\u00e7\u00f5es transacionais (OLTP) onde a maioria das consultas acessa todas as colunas de uma linha espec\u00edfica. Compress\u00e3o de Dados : ClickHouse : A compress\u00e3o de dados em colunas \u00e9 mais eficiente porque dados semelhantes s\u00e3o armazenados juntos, permitindo algoritmos de compress\u00e3o mais eficazes. PostgreSQL : Embora suporte compress\u00e3o, a efici\u00eancia \u00e9 menor devido ao armazenamento em linhas. Processamento em Modo Vetorial : ClickHouse : Utiliza processamento em modo vetorial, onde as opera\u00e7\u00f5es s\u00e3o realizadas em blocos de dados ao inv\u00e9s de tuplas individuais, resultando em melhor utiliza\u00e7\u00e3o da CPU e maior throughput. PostgreSQL : Processa dados em tuplas individuais, o que \u00e9 menos eficiente para grandes volumes de dados.","title":"Arquitetura e Design"},{"location":"arquitetura1/#performance-em-consultas","text":"Consultas Anal\u00edticas : ClickHouse : Projetado especificamente para consultas anal\u00edticas (OLAP), oferecendo desempenho superior em agrega\u00e7\u00f5es, filtragens e c\u00e1lculos em grandes conjuntos de dados. PostgreSQL : Embora possa executar consultas anal\u00edticas, n\u00e3o \u00e9 otimizado para tal e pode ser significativamente mais lento em compara\u00e7\u00e3o com ClickHouse. Indexa\u00e7\u00e3o : ClickHouse : Utiliza \u00edndices prim\u00e1rios baseados em ordena\u00e7\u00e3o e \u00edndices de dados em colunas, otimizados para leitura r\u00e1pida. PostgreSQL : Utiliza \u00edndices B-tree e outros tipos de \u00edndices que s\u00e3o mais vers\u00e1teis, mas podem ser menos eficientes para consultas anal\u00edticas em grandes volumes de dados.","title":"Performance em Consultas"},{"location":"arquitetura1/#trade-offs","text":"Transa\u00e7\u00f5es e Consist\u00eancia : ClickHouse : N\u00e3o oferece suporte completo a transa\u00e7\u00f5es ACID (Atomicidade, Consist\u00eancia, Isolamento e Durabilidade). \u00c9 otimizado para leituras r\u00e1pidas e escritas em massa, mas pode n\u00e3o garantir a consist\u00eancia em todas as opera\u00e7\u00f5es. PostgreSQL : Oferece suporte completo a transa\u00e7\u00f5es ACID, sendo ideal para aplica\u00e7\u00f5es OLTP onde a consist\u00eancia e a integridade dos dados s\u00e3o cruciais. Flexibilidade e Funcionalidades : ClickHouse : Focado em consultas anal\u00edticas, pode n\u00e3o oferecer todas as funcionalidades avan\u00e7adas de um banco de dados relacional completo, como suporte a joins complexos, triggers e stored procedures. PostgreSQL : Extremamente vers\u00e1til, oferece uma ampla gama de funcionalidades, incluindo suporte a joins complexos, triggers, stored procedures, e extens\u00f5es como PostGIS para dados geoespaciais. Escritas e Atualiza\u00e7\u00f5es : ClickHouse : Otimizado para leituras r\u00e1pidas, mas as opera\u00e7\u00f5es de escrita e atualiza\u00e7\u00e3o podem ser menos eficientes. N\u00e3o \u00e9 ideal para cargas de trabalho que envolvem muitas atualiza\u00e7\u00f5es frequentes de dados. PostgreSQL : Melhor equilibrado para opera\u00e7\u00f5es de leitura e escrita, sendo adequado tanto para OLTP quanto para OLAP, embora n\u00e3o seja t\u00e3o r\u00e1pido quanto ClickHouse para grandes consultas anal\u00edticas. Ecossistema e Suporte : ClickHouse : Comunidade em crescimento, mas ainda mais recente em compara\u00e7\u00e3o com PostgreSQL. Pode haver menos ferramentas e bibliotecas dispon\u00edveis. PostgreSQL : Ecossistema maduro com uma vasta gama de ferramentas, bibliotecas e suporte comunit\u00e1rio e comercial.","title":"Trade-offs"},{"location":"arquitetura1/#introducao-ao-parquet","text":"Parquet \u00e9 um formato de armazenamento de dados em colunas desenvolvido para o ecossistema Hadoop. \u00c9 projetado para ser altamente eficiente tanto em termos de espa\u00e7o quanto de desempenho, especialmente para grandes volumes de dados. Ele \u00e9 amplamente utilizado em ambientes de Big Data devido \u00e0 sua capacidade de suportar leitura e escrita eficientes.","title":"Introdu\u00e7\u00e3o ao Parquet"},{"location":"arquitetura1/#introducao-ao-csv","text":"CSV (Comma-Separated Values) \u00e9 um formato de arquivo simples e amplamente utilizado para armazenar dados tabulares em texto plano. Cada linha do arquivo representa um registro e cada campo \u00e9 separado por uma v\u00edrgula (ou outro delimitador, como ponto e v\u00edrgula).","title":"Introdu\u00e7\u00e3o ao CSV"},{"location":"arquitetura1/#comparacao-parquet-vs-csv","text":"","title":"Compara\u00e7\u00e3o: Parquet vs. CSV"},{"location":"arquitetura1/#estrutura-de-armazenamento","text":"Parquet : Armazenamento em Colunas : Os dados s\u00e3o armazenados por coluna, o que permite uma leitura e compress\u00e3o mais eficiente, especialmente para consultas que acessam um subconjunto de colunas. Metadados : Inclui metadados ricos que descrevem o esquema dos dados, tipos de dados e outras informa\u00e7\u00f5es \u00fateis. CSV : Armazenamento em Linhas : Os dados s\u00e3o armazenados por linha, o que \u00e9 simples e direto, mas menos eficiente para grandes volumes de dados e consultas que acessam apenas algumas colunas. Metadados : N\u00e3o possui metadados embutidos. O esquema dos dados deve ser inferido ou fornecido separadamente.","title":"Estrutura de Armazenamento"},{"location":"arquitetura1/#desempenho","text":"Parquet : Leitura e Escrita : Muito eficiente para leitura de grandes volumes de dados, especialmente quando apenas algumas colunas s\u00e3o necess\u00e1rias. A escrita pode ser mais lenta devido \u00e0 compress\u00e3o e ao armazenamento em colunas. Compress\u00e3o : Suporta compress\u00e3o eficiente de dados, reduzindo significativamente o espa\u00e7o de armazenamento necess\u00e1rio. Consultas Anal\u00edticas : Excelente desempenho para consultas anal\u00edticas devido ao armazenamento em colunas. CSV : Leitura e Escrita : Leitura e escrita simples e r\u00e1pidas, mas pode ser ineficiente para grandes volumes de dados. Compress\u00e3o : N\u00e3o suporta compress\u00e3o nativamente. Pode ser comprimido usando ferramentas externas (por exemplo, gzip), mas isso n\u00e3o \u00e9 t\u00e3o eficiente quanto a compress\u00e3o nativa do Parquet. Consultas Anal\u00edticas : Desempenho inferior para consultas anal\u00edticas em grandes volumes de dados devido ao armazenamento em linhas.","title":"Desempenho"},{"location":"arquitetura1/#flexibilidade-e-usabilidade","text":"Parquet : Complexidade : Mais complexo de usar e configurar devido \u00e0 necessidade de bibliotecas espec\u00edficas para leitura e escrita. Compatibilidade : Amplamente suportado em ferramentas de Big Data, como Apache Spark, Apache Hive, e Apache Drill. CSV : Simplicidade : Muito simples de usar e entender. Pode ser aberto e editado com qualquer editor de texto ou planilha. Compatibilidade : Suportado por praticamente todas as ferramentas de software, desde simples editores de texto at\u00e9 sistemas de banco de dados e ferramentas de an\u00e1lise de dados.","title":"Flexibilidade e Usabilidade"},{"location":"arquitetura1/#casos-de-uso","text":"Parquet : Ideal para grandes volumes de dados e cargas de trabalho anal\u00edticas. Usado em ambientes de Big Data onde a efici\u00eancia de armazenamento e desempenho de leitura s\u00e3o cr\u00edticos. CSV : Adequado para pequenos a m\u00e9dios volumes de dados. Ideal para interc\u00e2mbio de dados entre sistemas e para uso em ferramentas que n\u00e3o suportam formatos de dados mais complexos.","title":"Casos de Uso"},{"location":"arquitetura1/#para-o-nosso-data-lake-em-storages-do-supabase","text":"install.packages ( \"arrow\" ) library ( arrow ) # Caminho para o arquivo CSV csv_file <- \"caminho/para/seu/arquivo.csv\" # Caminho para o arquivo Parquet de sa\u00edda parquet_file <- \"caminho/para/seu/arquivo.parquet\" # Ler o arquivo CSV data <- read.csv ( csv_file ) # Escrever o arquivo Parquet write_parquet ( data , parquet_file ) cat ( \"Arquivo CSV transformado em Parquet com sucesso!\\n\" ) pip install pandas pyarrow import pandas as pd # Caminho para o arquivo CSV csv_file = \"caminho/para/seu/arquivo.csv\" # Caminho para o arquivo Parquet de sa\u00edda parquet_file = \"caminho/para/seu/arquivo.parquet\" # Ler o arquivo CSV df = pd . read_csv ( csv_file ) # Escrever o arquivo Parquet df . to_parquet ( parquet_file , engine = 'pyarrow' ) print ( \"Arquivo CSV transformado em Parquet com sucesso!\" )","title":"Para o Nosso Data Lake em Storages do Supabase:"},{"location":"arquitetura1/#exemplo-de-arquitetura-visual-grupo-01-do-curso-de-si-do-inteli-em-20234","text":"","title":"Exemplo de Arquitetura Visual -&gt; Grupo 01 do Curso de SI do Inteli em 2023.4"},{"location":"arquitetura2/","text":"","title":"Arquitetura e Governan\u00e7a de Dados na Nuvem 2"},{"location":"datalakes/","text":"","title":"Data Lakes e Data Warehouses"},{"location":"datasourcing/","text":"Data Sourcing e Data Analysis com R Studio \u00b6 Autoestudos \u00b6 Nome Tipo Notebook de Datasourcing Documenta\u00e7\u00e3o Exploratory Data Analysis in R Programming Documenta\u00e7\u00e3o Slides \u00b6 Introdu\u00e7\u00e3o \u00e0 Linguagem R \u00b6 O que \u00e9 R? \u00b6 R \u00e9 uma linguagem de programa\u00e7\u00e3o e um ambiente de software livre para computa\u00e7\u00e3o estat\u00edstica e gr\u00e1ficos. Foi desenvolvido por Ross Ihaka e Robert Gentleman na Universidade de Auckland, Nova Zel\u00e2ndia, e \u00e9 amplamente utilizado por estat\u00edsticos e analistas de dados para desenvolver software estat\u00edstico e realizar an\u00e1lise de dados. Caracter\u00edsticas Principais \u00b6 Interativa : R \u00e9 uma linguagem interpretada, o que significa que voc\u00ea pode executar comandos um de cada vez e ver os resultados imediatamente. Extens\u00edvel : H\u00e1 uma vasta quantidade de pacotes adicionais dispon\u00edveis que podem ser instalados para estender a funcionalidade do R. Gr\u00e1ficos de Alta Qualidade : R possui ferramentas poderosas para a cria\u00e7\u00e3o de gr\u00e1ficos de alta qualidade e visualiza\u00e7\u00f5es de dados. Comunidade Ativa : A comunidade R \u00e9 grande e ativa, oferecendo suporte e recursos atrav\u00e9s de f\u00f3runs, blogs, e documenta\u00e7\u00e3o. Estrutura B\u00e1sica da Linguagem \u00b6 Objetos e Vari\u00e1veis \u00b6 R \u00e9 uma linguagem orientada a objetos, e quase tudo em R \u00e9 um objeto. Voc\u00ea pode criar vari\u00e1veis para armazenar dados e manipular esses dados usando v\u00e1rias fun\u00e7\u00f5es. # Atribui\u00e7\u00e3o de valores a vari\u00e1veis x <- 10 y <- 5 # Opera\u00e7\u00f5es aritm\u00e9ticas z <- x + y print ( z ) # Sa\u00edda: 15 Tipos de Dados \u00b6 R suporta v\u00e1rios tipos de dados b\u00e1sicos, incluindo: N\u00fameros : inteiros e n\u00fameros de ponto flutuante. Caracteres : strings. Vetores : uma sequ\u00eancia de dados do mesmo tipo. Matrizes : uma cole\u00e7\u00e3o bidimensional de dados. Data Frames : uma tabela de dados, onde cada coluna pode conter diferentes tipos de dados. Listas : uma cole\u00e7\u00e3o de objetos que podem ser de tipos diferentes. # Exemplos de tipos de dados num <- 42 char <- \"Ol\u00e1, mundo!\" vetor <- c ( 1 , 2 , 3 , 4 , 5 ) matriz <- matrix ( 1 : 9 , nrow = 3 ) df <- data.frame ( Nomes = c ( \"Ana\" , \"Pedro\" ), Idades = c ( 28 , 34 )) lista <- list ( num , char , vetor , matriz , df ) Fun\u00e7\u00f5es \u00b6 Fun\u00e7\u00f5es s\u00e3o uma parte essencial de R e permitem encapsular c\u00f3digo que pode ser reutilizado. Voc\u00ea pode usar fun\u00e7\u00f5es integradas ou criar suas pr\u00f3prias fun\u00e7\u00f5es. # Fun\u00e7\u00e3o simples para somar dois n\u00fameros soma <- function ( a , b ) { return ( a + b ) } resultado <- soma ( 3 , 4 ) print ( resultado ) # Sa\u00edda: 7 Controle de Fluxo \u00b6 R suporta estruturas de controle de fluxo, como condicionais ( if , else ) e loops ( for , while ). # Condicional if-else if ( x > y ) { print ( \"x \u00e9 maior que y\" ) } else { print ( \"x n\u00e3o \u00e9 maior que y\" ) } # Loop for for ( i in 1 : 5 ) { print ( i ) } Importa\u00e7\u00e3o e Manipula\u00e7\u00e3o de Dados \u00b6 Importa\u00e7\u00e3o de Dados \u00b6 R facilita a importa\u00e7\u00e3o de dados de v\u00e1rias fontes, como arquivos CSV, Excel, bancos de dados e APIs. # Importar dados de um arquivo CSV dados <- read.csv ( \"dados.csv\" ) # Visualizar as primeiras linhas do data frame head ( dados ) Manipula\u00e7\u00e3o de Dados \u00b6 A manipula\u00e7\u00e3o de dados em R pode ser realizada usando pacotes como dplyr e tidyr . # Carregar o pacote dplyr library ( dplyr ) # Selecionar colunas e filtrar linhas dados_filtrados <- dados %>% select ( Nome , Idade ) %>% filter ( Idade > 30 ) Visualiza\u00e7\u00e3o de Dados \u00b6 R possui pacotes poderosos para visualiza\u00e7\u00e3o de dados, como ggplot2 . # Carregar o pacote ggplot2 library ( ggplot2 ) # Criar um gr\u00e1fico de dispers\u00e3o ggplot ( dados , aes ( x = Idade , y = Salario )) + geom_point () + theme_minimal () + labs ( title = \"Idade vs. Sal\u00e1rio\" , x = \"Idade\" , y = \"Sal\u00e1rio\" ) Pacotes e Extens\u00f5es \u00b6 Uma das grandes vantagens de R \u00e9 a sua extensibilidade atrav\u00e9s de pacotes. Voc\u00ea pode instalar pacotes do CRAN (Comprehensive R Archive Network) para adicionar novas funcionalidades ao R. # Instalar e carregar um pacote install.packages ( \"tidyverse\" ) library ( tidyverse ) An\u00e1lise Explorat\u00f3ria Avan\u00e7ada com R \u00b6 Pr\u00e9-requisitos \u00b6 Conhecimento b\u00e1sico de R e RStudio Conhecimento b\u00e1sico de EDA Conte\u00fado da Aula \u00b6 Prepara\u00e7\u00e3o do Ambiente Importa\u00e7\u00e3o e Limpeza de Dados An\u00e1lise Univariada An\u00e1lise Bivariada An\u00e1lise Multivariada Visualiza\u00e7\u00e3o Avan\u00e7ada Estat\u00edsticas Avan\u00e7adas Prepara\u00e7\u00e3o do Ambiente \u00b6 Para come\u00e7ar, precisamos preparar nosso ambiente de trabalho instalando e carregando os pacotes necess\u00e1rios. # Instalar pacotes necess\u00e1rios install.packages ( c ( \"tidyverse\" , \"data.table\" , \"GGally\" , \"corrplot\" , \"gridExtra\" )) # Carregar pacotes library ( tidyverse ) library ( data.table ) library ( GGally ) library ( corrplot ) library ( gridExtra ) Importa\u00e7\u00e3o e Limpeza de Dados \u00b6 Nesta se\u00e7\u00e3o, vamos importar nossos dados e realizar a limpeza necess\u00e1ria. # Importar dados data <- fread ( \"path/to/your/data.csv\" ) # Exibir as primeiras linhas dos dados head ( data ) # Verificar dados faltantes sum ( is.na ( data )) # Limpeza de dados (exemplo: remover linhas com dados faltantes) data <- na.omit ( data ) An\u00e1lise Univariada \u00b6 A an\u00e1lise univariada envolve a descri\u00e7\u00e3o de cada vari\u00e1vel individualmente. # Distribui\u00e7\u00e3o de uma vari\u00e1vel num\u00e9rica ggplot ( data , aes ( x = variable )) + geom_histogram ( binwidth = 10 , fill = \"blue\" , color = \"black\" ) + theme_minimal () + labs ( title = \"Distribui\u00e7\u00e3o de Variable\" , x = \"Variable\" , y = \"Frequ\u00eancia\" ) # Medidas de tend\u00eancia central e dispers\u00e3o summary ( data $ variable ) An\u00e1lise Bivariada \u00b6 A an\u00e1lise bivariada examina a rela\u00e7\u00e3o entre duas vari\u00e1veis. # Scatter plot para duas vari\u00e1veis num\u00e9ricas ggplot ( data , aes ( x = variable1 , y = variable2 )) + geom_point () + theme_minimal () + labs ( title = \"Rela\u00e7\u00e3o entre Variable1 e Variable2\" , x = \"Variable1\" , y = \"Variable2\" ) # Boxplot para uma vari\u00e1vel categ\u00f3rica e uma num\u00e9rica ggplot ( data , aes ( x = categorical_variable , y = numeric_variable )) + geom_boxplot () + theme_minimal () + labs ( title = \"Boxplot de Categorical Variable e Numeric Variable\" , x = \"Categorical Variable\" , y = \"Numeric Variable\" ) An\u00e1lise Multivariada \u00b6 A an\u00e1lise multivariada considera m\u00faltiplas vari\u00e1veis simultaneamente. # Matriz de correla\u00e7\u00e3o cor_matrix <- cor ( data %>% select_if ( is.numeric )) corrplot ( cor_matrix , method = \"circle\" ) # Scatterplot matrix ggpairs ( data %>% select_if ( is.numeric )) Visualiza\u00e7\u00e3o Avan\u00e7ada \u00b6 Vamos explorar t\u00e9cnicas avan\u00e7adas de visualiza\u00e7\u00e3o para EDA. # Densidade 2D para grandes conjuntos de dados ggplot ( data , aes ( x = variable1 , y = variable2 )) + geom_bin2d () + scale_fill_gradient ( low = \"white\" , high = \"blue\" ) + theme_minimal () + labs ( title = \"Densidade 2D de Variable1 e Variable2\" , x = \"Variable1\" , y = \"Variable2\" ) # Facet wrap para visualiza\u00e7\u00e3o de subgrupos ggplot ( data , aes ( x = variable )) + geom_histogram ( binwidth = 10 , fill = \"blue\" , color = \"black\" ) + facet_wrap ( ~ categorical_variable ) + theme_minimal () + labs ( title = \"Distribui\u00e7\u00e3o de Variable por Categoria\" , x = \"Variable\" , y = \"Frequ\u00eancia\" ) Estat\u00edsticas Avan\u00e7adas \u00b6 Utilizaremos t\u00e9cnicas estat\u00edsticas avan\u00e7adas para aprofundar nossa an\u00e1lise. # Regress\u00e3o linear simples model <- lm ( variable2 ~ variable1 , data = data ) summary ( model ) # An\u00e1lise de componentes principais (PCA) pca <- prcomp ( data %>% select_if ( is.numeric ), scale = TRUE ) summary ( pca ) # Visualiza\u00e7\u00e3o dos componentes principais autoplot ( pca , data = data , colour = 'categorical_variable' ) Por que Fazer An\u00e1lise Explorat\u00f3ria Avan\u00e7ada com R em Dados Grandes? \u00b6 Introdu\u00e7\u00e3o \u00b6 A an\u00e1lise explorat\u00f3ria de dados (EDA) \u00e9 um passo fundamental no processo de an\u00e1lise de dados. Quando se trata de grandes volumes de dados, a import\u00e2ncia de uma EDA avan\u00e7ada se torna ainda mais crucial. A linguagem R \u00e9 uma das ferramentas mais poderosas e vers\u00e1teis dispon\u00edveis para realizar EDA em grandes conjuntos de dados. Aqui est\u00e3o alguns motivos pelos quais voc\u00ea deve considerar fazer an\u00e1lise explorat\u00f3ria avan\u00e7ada com R em dados grandes. 1. Ferramentas Poderosas de Manipula\u00e7\u00e3o de Dados \u00b6 dplyr e data.table \u00b6 R oferece pacotes como dplyr e data.table que s\u00e3o otimizados para manipula\u00e7\u00e3o de dados r\u00e1pida e eficiente, mesmo com conjuntos de dados grandes. Eles permitem realizar opera\u00e7\u00f5es complexas de transforma\u00e7\u00e3o de dados com uma sintaxe simples e intuitiva. # Exemplo de manipula\u00e7\u00e3o de dados com dplyr library ( dplyr ) dados_grandes %>% filter ( salario > 50000 ) %>% group_by ( departamento ) %>% summarise ( media_salario = mean ( salario , na.rm = TRUE )) 2. Visualiza\u00e7\u00e3o de Dados Escal\u00e1vel \u00b6 ggplot2 e plotly \u00b6 A visualiza\u00e7\u00e3o \u00e9 uma parte essencial da EDA. R, com pacotes como ggplot2 e plotly , permite criar visualiza\u00e7\u00f5es detalhadas e interativas que podem lidar com grandes volumes de dados. Estas visualiza\u00e7\u00f5es ajudam a identificar padr\u00f5es, tend\u00eancias e anomalias que n\u00e3o s\u00e3o facilmente vis\u00edveis em tabelas de dados. # Exemplo de visualiza\u00e7\u00e3o com ggplot2 library ( ggplot2 ) ggplot ( dados_grandes , aes ( x = idade , y = salario )) + geom_point ( alpha = 0.5 ) + theme_minimal () + labs ( title = \"Rela\u00e7\u00e3o entre Idade e Sal\u00e1rio\" ) 3. Suporte para Computa\u00e7\u00e3o Paralela \u00b6 parallel e future \u00b6 Para conjuntos de dados muito grandes, a computa\u00e7\u00e3o paralela pode acelerar significativamente o processamento. R possui suporte nativo para computa\u00e7\u00e3o paralela atrav\u00e9s de pacotes como parallel e future , permitindo distribuir tarefas em m\u00faltiplos n\u00facleos de CPU. # Exemplo de computa\u00e7\u00e3o paralela com future library ( future ) plan ( multisession , workers = 4 ) resultados <- future_lapply ( 1 : 10 , function ( x ) sum ( rnorm ( 1e6 ))) 4. Integra\u00e7\u00e3o com Big Data \u00b6 sparklyr e DBI \u00b6 R pode se integrar facilmente com tecnologias de Big Data como Apache Spark atrav\u00e9s do pacote sparklyr , e tamb\u00e9m com bancos de dados SQL atrav\u00e9s do pacote DBI . Isso permite que voc\u00ea realize EDA em dados armazenados em sistemas distribu\u00eddos ou em bancos de dados de grande escala. Exemplo de Integra\u00e7\u00e3o com Spark usando Docker \u00b6 Passo 1: Configurar o Ambiente Docker \u00b6 Crie um arquivo docker-compose.yml para configurar um cluster Spark com Hadoop. version : '3.7' services : spark-master : image : bde2020/spark-master:latest container_name : spark-master ports : - \"8080:8080\" - \"7077:7077\" environment : - INIT_DAEMON_STEP=setup_spark networks : - spark spark-worker-1 : image : bde2020/spark-worker:latest container_name : spark-worker-1 depends_on : - spark-master environment : - \"SPARK_MASTER=spark://spark-master:7077\" - INIT_DAEMON_STEP=setup_worker networks : - spark networks : spark : driver : bridge Passo 2: Iniciar o Cluster Spark \u00b6 Inicie o cluster Spark usando Docker Compose. docker-compose up -d Passo 3: Conectar ao Spark no R \u00b6 Utilize o pacote sparklyr para conectar ao cluster Spark. # Instalar e carregar pacotes necess\u00e1rios install.packages ( \"sparklyr\" ) library ( sparklyr ) # Conectar ao cluster Spark sc <- spark_connect ( master = \"spark://localhost:7077\" ) # Copiar dados para o Spark dados_spark <- copy_to ( sc , dados_grandes , \"dados_grandes_tbl\" ) # Realizar opera\u00e7\u00f5es no Spark dados_spark %>% filter ( salario > 50000 ) %>% group_by ( departamento ) %>% summarise ( media_salario = mean ( salario , na.rm = TRUE )) %>% collect () Passo 4: Desconectar do Spark \u00b6 Ap\u00f3s a an\u00e1lise, desconecte do cluster Spark. spark_disconnect ( sc ) 5. Capacidades Estat\u00edsticas Avan\u00e7adas \u00b6 caret e mlr \u00b6 R \u00e9 amplamente reconhecido por suas capacidades estat\u00edsticas e de machine learning. Pacotes como caret e mlr permitem realizar an\u00e1lises estat\u00edsticas avan\u00e7adas e aplicar algoritmos de machine learning para descobrir insights profundos em grandes volumes de dados. # Exemplo de an\u00e1lise com caret library ( caret ) modelo <- train ( salario ~ . , data = dados_grandes , method = \"lm\" ) print ( modelo ) Conclus\u00e3o \u00b6 Nesta aula, exploramos t\u00e9cnicas avan\u00e7adas de An\u00e1lise Explorat\u00f3ria de Dados (EDA) usando R. Aprendemos a preparar o ambiente, importar e limpar dados, realizar an\u00e1lises univariadas, bivariadas e multivariadas, criar visualiza\u00e7\u00f5es avan\u00e7adas e aplicar estat\u00edsticas avan\u00e7adas. Com essas habilidades, voc\u00ea estar\u00e1 bem equipado para realizar EDA de maneira eficaz e profunda. Refer\u00eancias \u00b6 Notebook de Datasourcing Exploratory Data Analysis in R Programming R for Data Science The Comprehensive R Archive Network (CRAN) R Documentation R for Data Science Data Manipulation with dplyr Efficient R programming with data.table ggplot2: Elegant Graphics for Data Analysis Apache Spark Integration with R M\u00e3o na Massa! - An\u00e1lise Explorat\u00f3ria de Dados com o Conjunto de Dados Wine \u00b6 Introdu\u00e7\u00e3o \u00b6 O conjunto de dados de vinhos wine \u00e9 amplamente utilizado para demonstra\u00e7\u00f5es de t\u00e9cnicas de an\u00e1lise de dados e machine learning. Ele cont\u00e9m informa\u00e7\u00f5es qu\u00edmicas sobre diferentes tipos de vinhos e a qualidade atribu\u00edda a cada um. Neste tutorial, vamos realizar uma An\u00e1lise Explorat\u00f3ria de Dados (EDA) completa utilizando a linguagem R. Passo 1: Prepara\u00e7\u00e3o do Ambiente \u00b6 Primeiro, vamos preparar nosso ambiente de trabalho, instalando e carregando os pacotes necess\u00e1rios. # Instalar pacotes necess\u00e1rios install.packages ( c ( \"tidyverse\" , \"GGally\" , \"corrplot\" , \"gridExtra\" )) # Carregar pacotes library ( tidyverse ) library ( GGally ) library ( corrplot ) library ( gridExtra ) Passo 2: Carregar os Dados \u00b6 Vamos carregar o conjunto de dados wine . Supondo que o arquivo esteja em formato CSV, usaremos a fun\u00e7\u00e3o read.csv . # Carregar os dados wine_data <- read.csv ( \"path/to/wine.csv\" ) # Exibir as primeiras linhas do data frame head ( wine_data ) Passo 3: Entender a Estrutura dos Dados \u00b6 Examinaremos a estrutura dos dados para entender suas caracter\u00edsticas b\u00e1sicas. # Estrutura dos dados str ( wine_data ) # Resumo estat\u00edstico das vari\u00e1veis summary ( wine_data ) Passo 4: Verificar Dados Faltantes \u00b6 Verificaremos se h\u00e1 dados faltantes no conjunto de dados. # Verificar dados faltantes sum ( is.na ( wine_data )) Passo 5: An\u00e1lise Univariada \u00b6 Realizaremos a an\u00e1lise de cada vari\u00e1vel individualmente. Distribui\u00e7\u00e3o da Qualidade do Vinho \u00b6 # Histograma da vari\u00e1vel 'quality' ggplot ( wine_data , aes ( x = quality )) + geom_histogram ( binwidth = 1 , fill = \"blue\" , color = \"black\" ) + theme_minimal () + labs ( title = \"Distribui\u00e7\u00e3o da Qualidade do Vinho\" , x = \"Qualidade\" , y = \"Frequ\u00eancia\" ) Distribui\u00e7\u00e3o de Vari\u00e1veis Num\u00e9ricas \u00b6 # Histograma para cada vari\u00e1vel num\u00e9rica numeric_vars <- wine_data %>% select ( - quality ) p <- lapply ( names ( numeric_vars ), function ( var ) { ggplot ( wine_data , aes_string ( x = var )) + geom_histogram ( fill = \"blue\" , color = \"black\" , bins = 30 ) + theme_minimal () + labs ( title = paste ( \"Distribui\u00e7\u00e3o de\" , var ), x = var , y = \"Frequ\u00eancia\" ) }) grid.arrange ( grobs = p , ncol = 3 ) Passo 6: An\u00e1lise Bivariada \u00b6 Examinaremos as rela\u00e7\u00f5es entre pares de vari\u00e1veis. Rela\u00e7\u00e3o entre \u00c1lcool e Qualidade \u00b6 # Scatter plot entre 'alcohol' e 'quality' ggplot ( wine_data , aes ( x = alcohol , y = quality )) + geom_point ( alpha = 0.5 ) + theme_minimal () + labs ( title = \"Rela\u00e7\u00e3o entre \u00c1lcool e Qualidade\" , x = \"\u00c1lcool\" , y = \"Qualidade\" ) Boxplot da Qualidade por Tipo de Vinho \u00b6 # Supondo que h\u00e1 uma coluna 'type' indicando o tipo de vinho ggplot ( wine_data , aes ( x = factor ( quality ), y = alcohol , fill = factor ( quality ))) + geom_boxplot () + theme_minimal () + labs ( title = \"Boxplot da Qualidade por Tipo de Vinho\" , x = \"Qualidade\" , y = \"\u00c1lcool\" ) Passo 7: An\u00e1lise Multivariada \u00b6 Analisaremos m\u00faltiplas vari\u00e1veis simultaneamente. Matriz de Correla\u00e7\u00e3o \u00b6 # Matriz de correla\u00e7\u00e3o cor_matrix <- cor ( wine_data %>% select_if ( is.numeric )) corrplot ( cor_matrix , method = \"circle\" ) Scatterplot Matrix \u00b6 # Scatterplot matrix ggpairs ( wine_data %>% select_if ( is.numeric )) Passo 8: Conclus\u00f5es e Insights \u00b6 Ap\u00f3s a an\u00e1lise, resumimos nossos principais achados: A distribui\u00e7\u00e3o da qualidade dos vinhos \u00e9... A rela\u00e7\u00e3o entre o teor de \u00e1lcool e a qualidade \u00e9... Vari\u00e1veis como \u00e1cido c\u00edtrico e pH apresentam correla\u00e7\u00e3o... Refer\u00eancias \u00b6 Documenta\u00e7\u00e3o do ggplot2 Guia de Introdu\u00e7\u00e3o ao dplyr Qual a Miss\u00e3o para o Artefato? \u00b6 Os grupos dever\u00e3o realizar uma An\u00e1lise Explorat\u00f3ria Avan\u00e7ada nos dados que foram fornecidos. Esta an\u00e1lise deve incluir, mas n\u00e3o se limitar a: Entender a estrutura dos dados e suas caracter\u00edsticas b\u00e1sicas. Realizar uma an\u00e1lise univariada de todas as vari\u00e1veis. Explorar rela\u00e7\u00f5es bivariadas entre vari\u00e1veis para identificar poss\u00edveis correla\u00e7\u00f5es. Conduzir uma an\u00e1lise multivariada para entender intera\u00e7\u00f5es complexas entre vari\u00e1veis. Utilizar visualiza\u00e7\u00f5es para ilustrar as descobertas e insights obtidos. Uso de Computa\u00e7\u00e3o Distribu\u00edda com Spark \u00b6 Caso o conjunto de dados seja muito extenso e torne o processamento em um \u00fanico computador invi\u00e1vel, os grupos dever\u00e3o utilizar o Apache Spark como solu\u00e7\u00e3o de computa\u00e7\u00e3o distribu\u00edda. O Spark permite manipular e processar grandes volumes de dados de maneira eficiente, distribuindo as tarefas em m\u00faltiplos n\u00f3s de computa\u00e7\u00e3o. Utilizem essas ferramentas e t\u00e9cnicas para garantir que a an\u00e1lise seja completa e eficiente, mesmo com conjuntos de dados volumosos. O objetivo final \u00e9 extrair insights valiosos dos dados fornecidos e apresentar esses insights de forma clara e compreens\u00edvel.","title":"Data Sourcing e Data Analysis com R Studio"},{"location":"datasourcing/#data-sourcing-e-data-analysis-com-r-studio","text":"","title":"Data Sourcing e Data Analysis com R Studio"},{"location":"datasourcing/#autoestudos","text":"Nome Tipo Notebook de Datasourcing Documenta\u00e7\u00e3o Exploratory Data Analysis in R Programming Documenta\u00e7\u00e3o","title":"Autoestudos"},{"location":"datasourcing/#slides","text":"","title":"Slides"},{"location":"datasourcing/#introducao-a-linguagem-r","text":"","title":"Introdu\u00e7\u00e3o \u00e0 Linguagem R"},{"location":"datasourcing/#o-que-e-r","text":"R \u00e9 uma linguagem de programa\u00e7\u00e3o e um ambiente de software livre para computa\u00e7\u00e3o estat\u00edstica e gr\u00e1ficos. Foi desenvolvido por Ross Ihaka e Robert Gentleman na Universidade de Auckland, Nova Zel\u00e2ndia, e \u00e9 amplamente utilizado por estat\u00edsticos e analistas de dados para desenvolver software estat\u00edstico e realizar an\u00e1lise de dados.","title":"O que \u00e9 R?"},{"location":"datasourcing/#caracteristicas-principais","text":"Interativa : R \u00e9 uma linguagem interpretada, o que significa que voc\u00ea pode executar comandos um de cada vez e ver os resultados imediatamente. Extens\u00edvel : H\u00e1 uma vasta quantidade de pacotes adicionais dispon\u00edveis que podem ser instalados para estender a funcionalidade do R. Gr\u00e1ficos de Alta Qualidade : R possui ferramentas poderosas para a cria\u00e7\u00e3o de gr\u00e1ficos de alta qualidade e visualiza\u00e7\u00f5es de dados. Comunidade Ativa : A comunidade R \u00e9 grande e ativa, oferecendo suporte e recursos atrav\u00e9s de f\u00f3runs, blogs, e documenta\u00e7\u00e3o.","title":"Caracter\u00edsticas Principais"},{"location":"datasourcing/#estrutura-basica-da-linguagem","text":"","title":"Estrutura B\u00e1sica da Linguagem"},{"location":"datasourcing/#objetos-e-variaveis","text":"R \u00e9 uma linguagem orientada a objetos, e quase tudo em R \u00e9 um objeto. Voc\u00ea pode criar vari\u00e1veis para armazenar dados e manipular esses dados usando v\u00e1rias fun\u00e7\u00f5es. # Atribui\u00e7\u00e3o de valores a vari\u00e1veis x <- 10 y <- 5 # Opera\u00e7\u00f5es aritm\u00e9ticas z <- x + y print ( z ) # Sa\u00edda: 15","title":"Objetos e Vari\u00e1veis"},{"location":"datasourcing/#tipos-de-dados","text":"R suporta v\u00e1rios tipos de dados b\u00e1sicos, incluindo: N\u00fameros : inteiros e n\u00fameros de ponto flutuante. Caracteres : strings. Vetores : uma sequ\u00eancia de dados do mesmo tipo. Matrizes : uma cole\u00e7\u00e3o bidimensional de dados. Data Frames : uma tabela de dados, onde cada coluna pode conter diferentes tipos de dados. Listas : uma cole\u00e7\u00e3o de objetos que podem ser de tipos diferentes. # Exemplos de tipos de dados num <- 42 char <- \"Ol\u00e1, mundo!\" vetor <- c ( 1 , 2 , 3 , 4 , 5 ) matriz <- matrix ( 1 : 9 , nrow = 3 ) df <- data.frame ( Nomes = c ( \"Ana\" , \"Pedro\" ), Idades = c ( 28 , 34 )) lista <- list ( num , char , vetor , matriz , df )","title":"Tipos de Dados"},{"location":"datasourcing/#funcoes","text":"Fun\u00e7\u00f5es s\u00e3o uma parte essencial de R e permitem encapsular c\u00f3digo que pode ser reutilizado. Voc\u00ea pode usar fun\u00e7\u00f5es integradas ou criar suas pr\u00f3prias fun\u00e7\u00f5es. # Fun\u00e7\u00e3o simples para somar dois n\u00fameros soma <- function ( a , b ) { return ( a + b ) } resultado <- soma ( 3 , 4 ) print ( resultado ) # Sa\u00edda: 7","title":"Fun\u00e7\u00f5es"},{"location":"datasourcing/#controle-de-fluxo","text":"R suporta estruturas de controle de fluxo, como condicionais ( if , else ) e loops ( for , while ). # Condicional if-else if ( x > y ) { print ( \"x \u00e9 maior que y\" ) } else { print ( \"x n\u00e3o \u00e9 maior que y\" ) } # Loop for for ( i in 1 : 5 ) { print ( i ) }","title":"Controle de Fluxo"},{"location":"datasourcing/#importacao-e-manipulacao-de-dados","text":"","title":"Importa\u00e7\u00e3o e Manipula\u00e7\u00e3o de Dados"},{"location":"datasourcing/#importacao-de-dados","text":"R facilita a importa\u00e7\u00e3o de dados de v\u00e1rias fontes, como arquivos CSV, Excel, bancos de dados e APIs. # Importar dados de um arquivo CSV dados <- read.csv ( \"dados.csv\" ) # Visualizar as primeiras linhas do data frame head ( dados )","title":"Importa\u00e7\u00e3o de Dados"},{"location":"datasourcing/#manipulacao-de-dados","text":"A manipula\u00e7\u00e3o de dados em R pode ser realizada usando pacotes como dplyr e tidyr . # Carregar o pacote dplyr library ( dplyr ) # Selecionar colunas e filtrar linhas dados_filtrados <- dados %>% select ( Nome , Idade ) %>% filter ( Idade > 30 )","title":"Manipula\u00e7\u00e3o de Dados"},{"location":"datasourcing/#visualizacao-de-dados","text":"R possui pacotes poderosos para visualiza\u00e7\u00e3o de dados, como ggplot2 . # Carregar o pacote ggplot2 library ( ggplot2 ) # Criar um gr\u00e1fico de dispers\u00e3o ggplot ( dados , aes ( x = Idade , y = Salario )) + geom_point () + theme_minimal () + labs ( title = \"Idade vs. Sal\u00e1rio\" , x = \"Idade\" , y = \"Sal\u00e1rio\" )","title":"Visualiza\u00e7\u00e3o de Dados"},{"location":"datasourcing/#pacotes-e-extensoes","text":"Uma das grandes vantagens de R \u00e9 a sua extensibilidade atrav\u00e9s de pacotes. Voc\u00ea pode instalar pacotes do CRAN (Comprehensive R Archive Network) para adicionar novas funcionalidades ao R. # Instalar e carregar um pacote install.packages ( \"tidyverse\" ) library ( tidyverse )","title":"Pacotes e Extens\u00f5es"},{"location":"datasourcing/#analise-exploratoria-avancada-com-r","text":"","title":"An\u00e1lise Explorat\u00f3ria Avan\u00e7ada com R"},{"location":"datasourcing/#pre-requisitos","text":"Conhecimento b\u00e1sico de R e RStudio Conhecimento b\u00e1sico de EDA","title":"Pr\u00e9-requisitos"},{"location":"datasourcing/#conteudo-da-aula","text":"Prepara\u00e7\u00e3o do Ambiente Importa\u00e7\u00e3o e Limpeza de Dados An\u00e1lise Univariada An\u00e1lise Bivariada An\u00e1lise Multivariada Visualiza\u00e7\u00e3o Avan\u00e7ada Estat\u00edsticas Avan\u00e7adas","title":"Conte\u00fado da Aula"},{"location":"datasourcing/#preparacao-do-ambiente","text":"Para come\u00e7ar, precisamos preparar nosso ambiente de trabalho instalando e carregando os pacotes necess\u00e1rios. # Instalar pacotes necess\u00e1rios install.packages ( c ( \"tidyverse\" , \"data.table\" , \"GGally\" , \"corrplot\" , \"gridExtra\" )) # Carregar pacotes library ( tidyverse ) library ( data.table ) library ( GGally ) library ( corrplot ) library ( gridExtra )","title":"Prepara\u00e7\u00e3o do Ambiente"},{"location":"datasourcing/#importacao-e-limpeza-de-dados","text":"Nesta se\u00e7\u00e3o, vamos importar nossos dados e realizar a limpeza necess\u00e1ria. # Importar dados data <- fread ( \"path/to/your/data.csv\" ) # Exibir as primeiras linhas dos dados head ( data ) # Verificar dados faltantes sum ( is.na ( data )) # Limpeza de dados (exemplo: remover linhas com dados faltantes) data <- na.omit ( data )","title":"Importa\u00e7\u00e3o e Limpeza de Dados"},{"location":"datasourcing/#analise-univariada","text":"A an\u00e1lise univariada envolve a descri\u00e7\u00e3o de cada vari\u00e1vel individualmente. # Distribui\u00e7\u00e3o de uma vari\u00e1vel num\u00e9rica ggplot ( data , aes ( x = variable )) + geom_histogram ( binwidth = 10 , fill = \"blue\" , color = \"black\" ) + theme_minimal () + labs ( title = \"Distribui\u00e7\u00e3o de Variable\" , x = \"Variable\" , y = \"Frequ\u00eancia\" ) # Medidas de tend\u00eancia central e dispers\u00e3o summary ( data $ variable )","title":"An\u00e1lise Univariada"},{"location":"datasourcing/#analise-bivariada","text":"A an\u00e1lise bivariada examina a rela\u00e7\u00e3o entre duas vari\u00e1veis. # Scatter plot para duas vari\u00e1veis num\u00e9ricas ggplot ( data , aes ( x = variable1 , y = variable2 )) + geom_point () + theme_minimal () + labs ( title = \"Rela\u00e7\u00e3o entre Variable1 e Variable2\" , x = \"Variable1\" , y = \"Variable2\" ) # Boxplot para uma vari\u00e1vel categ\u00f3rica e uma num\u00e9rica ggplot ( data , aes ( x = categorical_variable , y = numeric_variable )) + geom_boxplot () + theme_minimal () + labs ( title = \"Boxplot de Categorical Variable e Numeric Variable\" , x = \"Categorical Variable\" , y = \"Numeric Variable\" )","title":"An\u00e1lise Bivariada"},{"location":"datasourcing/#analise-multivariada","text":"A an\u00e1lise multivariada considera m\u00faltiplas vari\u00e1veis simultaneamente. # Matriz de correla\u00e7\u00e3o cor_matrix <- cor ( data %>% select_if ( is.numeric )) corrplot ( cor_matrix , method = \"circle\" ) # Scatterplot matrix ggpairs ( data %>% select_if ( is.numeric ))","title":"An\u00e1lise Multivariada"},{"location":"datasourcing/#visualizacao-avancada","text":"Vamos explorar t\u00e9cnicas avan\u00e7adas de visualiza\u00e7\u00e3o para EDA. # Densidade 2D para grandes conjuntos de dados ggplot ( data , aes ( x = variable1 , y = variable2 )) + geom_bin2d () + scale_fill_gradient ( low = \"white\" , high = \"blue\" ) + theme_minimal () + labs ( title = \"Densidade 2D de Variable1 e Variable2\" , x = \"Variable1\" , y = \"Variable2\" ) # Facet wrap para visualiza\u00e7\u00e3o de subgrupos ggplot ( data , aes ( x = variable )) + geom_histogram ( binwidth = 10 , fill = \"blue\" , color = \"black\" ) + facet_wrap ( ~ categorical_variable ) + theme_minimal () + labs ( title = \"Distribui\u00e7\u00e3o de Variable por Categoria\" , x = \"Variable\" , y = \"Frequ\u00eancia\" )","title":"Visualiza\u00e7\u00e3o Avan\u00e7ada"},{"location":"datasourcing/#estatisticas-avancadas","text":"Utilizaremos t\u00e9cnicas estat\u00edsticas avan\u00e7adas para aprofundar nossa an\u00e1lise. # Regress\u00e3o linear simples model <- lm ( variable2 ~ variable1 , data = data ) summary ( model ) # An\u00e1lise de componentes principais (PCA) pca <- prcomp ( data %>% select_if ( is.numeric ), scale = TRUE ) summary ( pca ) # Visualiza\u00e7\u00e3o dos componentes principais autoplot ( pca , data = data , colour = 'categorical_variable' )","title":"Estat\u00edsticas Avan\u00e7adas"},{"location":"datasourcing/#por-que-fazer-analise-exploratoria-avancada-com-r-em-dados-grandes","text":"","title":"Por que Fazer An\u00e1lise Explorat\u00f3ria Avan\u00e7ada com R em Dados Grandes?"},{"location":"datasourcing/#introducao","text":"A an\u00e1lise explorat\u00f3ria de dados (EDA) \u00e9 um passo fundamental no processo de an\u00e1lise de dados. Quando se trata de grandes volumes de dados, a import\u00e2ncia de uma EDA avan\u00e7ada se torna ainda mais crucial. A linguagem R \u00e9 uma das ferramentas mais poderosas e vers\u00e1teis dispon\u00edveis para realizar EDA em grandes conjuntos de dados. Aqui est\u00e3o alguns motivos pelos quais voc\u00ea deve considerar fazer an\u00e1lise explorat\u00f3ria avan\u00e7ada com R em dados grandes.","title":"Introdu\u00e7\u00e3o"},{"location":"datasourcing/#1-ferramentas-poderosas-de-manipulacao-de-dados","text":"","title":"1. Ferramentas Poderosas de Manipula\u00e7\u00e3o de Dados"},{"location":"datasourcing/#dplyr-e-datatable","text":"R oferece pacotes como dplyr e data.table que s\u00e3o otimizados para manipula\u00e7\u00e3o de dados r\u00e1pida e eficiente, mesmo com conjuntos de dados grandes. Eles permitem realizar opera\u00e7\u00f5es complexas de transforma\u00e7\u00e3o de dados com uma sintaxe simples e intuitiva. # Exemplo de manipula\u00e7\u00e3o de dados com dplyr library ( dplyr ) dados_grandes %>% filter ( salario > 50000 ) %>% group_by ( departamento ) %>% summarise ( media_salario = mean ( salario , na.rm = TRUE ))","title":"dplyr e data.table"},{"location":"datasourcing/#2-visualizacao-de-dados-escalavel","text":"","title":"2. Visualiza\u00e7\u00e3o de Dados Escal\u00e1vel"},{"location":"datasourcing/#ggplot2-e-plotly","text":"A visualiza\u00e7\u00e3o \u00e9 uma parte essencial da EDA. R, com pacotes como ggplot2 e plotly , permite criar visualiza\u00e7\u00f5es detalhadas e interativas que podem lidar com grandes volumes de dados. Estas visualiza\u00e7\u00f5es ajudam a identificar padr\u00f5es, tend\u00eancias e anomalias que n\u00e3o s\u00e3o facilmente vis\u00edveis em tabelas de dados. # Exemplo de visualiza\u00e7\u00e3o com ggplot2 library ( ggplot2 ) ggplot ( dados_grandes , aes ( x = idade , y = salario )) + geom_point ( alpha = 0.5 ) + theme_minimal () + labs ( title = \"Rela\u00e7\u00e3o entre Idade e Sal\u00e1rio\" )","title":"ggplot2 e plotly"},{"location":"datasourcing/#3-suporte-para-computacao-paralela","text":"","title":"3. Suporte para Computa\u00e7\u00e3o Paralela"},{"location":"datasourcing/#parallel-e-future","text":"Para conjuntos de dados muito grandes, a computa\u00e7\u00e3o paralela pode acelerar significativamente o processamento. R possui suporte nativo para computa\u00e7\u00e3o paralela atrav\u00e9s de pacotes como parallel e future , permitindo distribuir tarefas em m\u00faltiplos n\u00facleos de CPU. # Exemplo de computa\u00e7\u00e3o paralela com future library ( future ) plan ( multisession , workers = 4 ) resultados <- future_lapply ( 1 : 10 , function ( x ) sum ( rnorm ( 1e6 )))","title":"parallel e future"},{"location":"datasourcing/#4-integracao-com-big-data","text":"","title":"4. Integra\u00e7\u00e3o com Big Data"},{"location":"datasourcing/#sparklyr-e-dbi","text":"R pode se integrar facilmente com tecnologias de Big Data como Apache Spark atrav\u00e9s do pacote sparklyr , e tamb\u00e9m com bancos de dados SQL atrav\u00e9s do pacote DBI . Isso permite que voc\u00ea realize EDA em dados armazenados em sistemas distribu\u00eddos ou em bancos de dados de grande escala.","title":"sparklyr e DBI"},{"location":"datasourcing/#exemplo-de-integracao-com-spark-usando-docker","text":"","title":"Exemplo de Integra\u00e7\u00e3o com Spark usando Docker"},{"location":"datasourcing/#passo-1-configurar-o-ambiente-docker","text":"Crie um arquivo docker-compose.yml para configurar um cluster Spark com Hadoop. version : '3.7' services : spark-master : image : bde2020/spark-master:latest container_name : spark-master ports : - \"8080:8080\" - \"7077:7077\" environment : - INIT_DAEMON_STEP=setup_spark networks : - spark spark-worker-1 : image : bde2020/spark-worker:latest container_name : spark-worker-1 depends_on : - spark-master environment : - \"SPARK_MASTER=spark://spark-master:7077\" - INIT_DAEMON_STEP=setup_worker networks : - spark networks : spark : driver : bridge","title":"Passo 1: Configurar o Ambiente Docker"},{"location":"datasourcing/#passo-2-iniciar-o-cluster-spark","text":"Inicie o cluster Spark usando Docker Compose. docker-compose up -d","title":"Passo 2: Iniciar o Cluster Spark"},{"location":"datasourcing/#passo-3-conectar-ao-spark-no-r","text":"Utilize o pacote sparklyr para conectar ao cluster Spark. # Instalar e carregar pacotes necess\u00e1rios install.packages ( \"sparklyr\" ) library ( sparklyr ) # Conectar ao cluster Spark sc <- spark_connect ( master = \"spark://localhost:7077\" ) # Copiar dados para o Spark dados_spark <- copy_to ( sc , dados_grandes , \"dados_grandes_tbl\" ) # Realizar opera\u00e7\u00f5es no Spark dados_spark %>% filter ( salario > 50000 ) %>% group_by ( departamento ) %>% summarise ( media_salario = mean ( salario , na.rm = TRUE )) %>% collect ()","title":"Passo 3: Conectar ao Spark no R"},{"location":"datasourcing/#passo-4-desconectar-do-spark","text":"Ap\u00f3s a an\u00e1lise, desconecte do cluster Spark. spark_disconnect ( sc )","title":"Passo 4: Desconectar do Spark"},{"location":"datasourcing/#5-capacidades-estatisticas-avancadas","text":"","title":"5. Capacidades Estat\u00edsticas Avan\u00e7adas"},{"location":"datasourcing/#caret-e-mlr","text":"R \u00e9 amplamente reconhecido por suas capacidades estat\u00edsticas e de machine learning. Pacotes como caret e mlr permitem realizar an\u00e1lises estat\u00edsticas avan\u00e7adas e aplicar algoritmos de machine learning para descobrir insights profundos em grandes volumes de dados. # Exemplo de an\u00e1lise com caret library ( caret ) modelo <- train ( salario ~ . , data = dados_grandes , method = \"lm\" ) print ( modelo )","title":"caret e mlr"},{"location":"datasourcing/#conclusao","text":"Nesta aula, exploramos t\u00e9cnicas avan\u00e7adas de An\u00e1lise Explorat\u00f3ria de Dados (EDA) usando R. Aprendemos a preparar o ambiente, importar e limpar dados, realizar an\u00e1lises univariadas, bivariadas e multivariadas, criar visualiza\u00e7\u00f5es avan\u00e7adas e aplicar estat\u00edsticas avan\u00e7adas. Com essas habilidades, voc\u00ea estar\u00e1 bem equipado para realizar EDA de maneira eficaz e profunda.","title":"Conclus\u00e3o"},{"location":"datasourcing/#referencias","text":"Notebook de Datasourcing Exploratory Data Analysis in R Programming R for Data Science The Comprehensive R Archive Network (CRAN) R Documentation R for Data Science Data Manipulation with dplyr Efficient R programming with data.table ggplot2: Elegant Graphics for Data Analysis Apache Spark Integration with R","title":"Refer\u00eancias"},{"location":"datasourcing/#mao-na-massa-analise-exploratoria-de-dados-com-o-conjunto-de-dados-wine","text":"","title":"M\u00e3o na Massa! - An\u00e1lise Explorat\u00f3ria de Dados com o Conjunto de Dados Wine"},{"location":"datasourcing/#introducao_1","text":"O conjunto de dados de vinhos wine \u00e9 amplamente utilizado para demonstra\u00e7\u00f5es de t\u00e9cnicas de an\u00e1lise de dados e machine learning. Ele cont\u00e9m informa\u00e7\u00f5es qu\u00edmicas sobre diferentes tipos de vinhos e a qualidade atribu\u00edda a cada um. Neste tutorial, vamos realizar uma An\u00e1lise Explorat\u00f3ria de Dados (EDA) completa utilizando a linguagem R.","title":"Introdu\u00e7\u00e3o"},{"location":"datasourcing/#passo-1-preparacao-do-ambiente","text":"Primeiro, vamos preparar nosso ambiente de trabalho, instalando e carregando os pacotes necess\u00e1rios. # Instalar pacotes necess\u00e1rios install.packages ( c ( \"tidyverse\" , \"GGally\" , \"corrplot\" , \"gridExtra\" )) # Carregar pacotes library ( tidyverse ) library ( GGally ) library ( corrplot ) library ( gridExtra )","title":"Passo 1: Prepara\u00e7\u00e3o do Ambiente"},{"location":"datasourcing/#passo-2-carregar-os-dados","text":"Vamos carregar o conjunto de dados wine . Supondo que o arquivo esteja em formato CSV, usaremos a fun\u00e7\u00e3o read.csv . # Carregar os dados wine_data <- read.csv ( \"path/to/wine.csv\" ) # Exibir as primeiras linhas do data frame head ( wine_data )","title":"Passo 2: Carregar os Dados"},{"location":"datasourcing/#passo-3-entender-a-estrutura-dos-dados","text":"Examinaremos a estrutura dos dados para entender suas caracter\u00edsticas b\u00e1sicas. # Estrutura dos dados str ( wine_data ) # Resumo estat\u00edstico das vari\u00e1veis summary ( wine_data )","title":"Passo 3: Entender a Estrutura dos Dados"},{"location":"datasourcing/#passo-4-verificar-dados-faltantes","text":"Verificaremos se h\u00e1 dados faltantes no conjunto de dados. # Verificar dados faltantes sum ( is.na ( wine_data ))","title":"Passo 4: Verificar Dados Faltantes"},{"location":"datasourcing/#passo-5-analise-univariada","text":"Realizaremos a an\u00e1lise de cada vari\u00e1vel individualmente.","title":"Passo 5: An\u00e1lise Univariada"},{"location":"datasourcing/#distribuicao-da-qualidade-do-vinho","text":"# Histograma da vari\u00e1vel 'quality' ggplot ( wine_data , aes ( x = quality )) + geom_histogram ( binwidth = 1 , fill = \"blue\" , color = \"black\" ) + theme_minimal () + labs ( title = \"Distribui\u00e7\u00e3o da Qualidade do Vinho\" , x = \"Qualidade\" , y = \"Frequ\u00eancia\" )","title":"Distribui\u00e7\u00e3o da Qualidade do Vinho"},{"location":"datasourcing/#distribuicao-de-variaveis-numericas","text":"# Histograma para cada vari\u00e1vel num\u00e9rica numeric_vars <- wine_data %>% select ( - quality ) p <- lapply ( names ( numeric_vars ), function ( var ) { ggplot ( wine_data , aes_string ( x = var )) + geom_histogram ( fill = \"blue\" , color = \"black\" , bins = 30 ) + theme_minimal () + labs ( title = paste ( \"Distribui\u00e7\u00e3o de\" , var ), x = var , y = \"Frequ\u00eancia\" ) }) grid.arrange ( grobs = p , ncol = 3 )","title":"Distribui\u00e7\u00e3o de Vari\u00e1veis Num\u00e9ricas"},{"location":"datasourcing/#passo-6-analise-bivariada","text":"Examinaremos as rela\u00e7\u00f5es entre pares de vari\u00e1veis.","title":"Passo 6: An\u00e1lise Bivariada"},{"location":"datasourcing/#relacao-entre-alcool-e-qualidade","text":"# Scatter plot entre 'alcohol' e 'quality' ggplot ( wine_data , aes ( x = alcohol , y = quality )) + geom_point ( alpha = 0.5 ) + theme_minimal () + labs ( title = \"Rela\u00e7\u00e3o entre \u00c1lcool e Qualidade\" , x = \"\u00c1lcool\" , y = \"Qualidade\" )","title":"Rela\u00e7\u00e3o entre \u00c1lcool e Qualidade"},{"location":"datasourcing/#boxplot-da-qualidade-por-tipo-de-vinho","text":"# Supondo que h\u00e1 uma coluna 'type' indicando o tipo de vinho ggplot ( wine_data , aes ( x = factor ( quality ), y = alcohol , fill = factor ( quality ))) + geom_boxplot () + theme_minimal () + labs ( title = \"Boxplot da Qualidade por Tipo de Vinho\" , x = \"Qualidade\" , y = \"\u00c1lcool\" )","title":"Boxplot da Qualidade por Tipo de Vinho"},{"location":"datasourcing/#passo-7-analise-multivariada","text":"Analisaremos m\u00faltiplas vari\u00e1veis simultaneamente.","title":"Passo 7: An\u00e1lise Multivariada"},{"location":"datasourcing/#matriz-de-correlacao","text":"# Matriz de correla\u00e7\u00e3o cor_matrix <- cor ( wine_data %>% select_if ( is.numeric )) corrplot ( cor_matrix , method = \"circle\" )","title":"Matriz de Correla\u00e7\u00e3o"},{"location":"datasourcing/#scatterplot-matrix","text":"# Scatterplot matrix ggpairs ( wine_data %>% select_if ( is.numeric ))","title":"Scatterplot Matrix"},{"location":"datasourcing/#passo-8-conclusoes-e-insights","text":"Ap\u00f3s a an\u00e1lise, resumimos nossos principais achados: A distribui\u00e7\u00e3o da qualidade dos vinhos \u00e9... A rela\u00e7\u00e3o entre o teor de \u00e1lcool e a qualidade \u00e9... Vari\u00e1veis como \u00e1cido c\u00edtrico e pH apresentam correla\u00e7\u00e3o...","title":"Passo 8: Conclus\u00f5es e Insights"},{"location":"datasourcing/#referencias_1","text":"Documenta\u00e7\u00e3o do ggplot2 Guia de Introdu\u00e7\u00e3o ao dplyr","title":"Refer\u00eancias"},{"location":"datasourcing/#qual-a-missao-para-o-artefato","text":"Os grupos dever\u00e3o realizar uma An\u00e1lise Explorat\u00f3ria Avan\u00e7ada nos dados que foram fornecidos. Esta an\u00e1lise deve incluir, mas n\u00e3o se limitar a: Entender a estrutura dos dados e suas caracter\u00edsticas b\u00e1sicas. Realizar uma an\u00e1lise univariada de todas as vari\u00e1veis. Explorar rela\u00e7\u00f5es bivariadas entre vari\u00e1veis para identificar poss\u00edveis correla\u00e7\u00f5es. Conduzir uma an\u00e1lise multivariada para entender intera\u00e7\u00f5es complexas entre vari\u00e1veis. Utilizar visualiza\u00e7\u00f5es para ilustrar as descobertas e insights obtidos.","title":"Qual a Miss\u00e3o para o Artefato?"},{"location":"datasourcing/#uso-de-computacao-distribuida-com-spark","text":"Caso o conjunto de dados seja muito extenso e torne o processamento em um \u00fanico computador invi\u00e1vel, os grupos dever\u00e3o utilizar o Apache Spark como solu\u00e7\u00e3o de computa\u00e7\u00e3o distribu\u00edda. O Spark permite manipular e processar grandes volumes de dados de maneira eficiente, distribuindo as tarefas em m\u00faltiplos n\u00f3s de computa\u00e7\u00e3o. Utilizem essas ferramentas e t\u00e9cnicas para garantir que a an\u00e1lise seja completa e eficiente, mesmo com conjuntos de dados volumosos. O objetivo final \u00e9 extrair insights valiosos dos dados fornecidos e apresentar esses insights de forma clara e compreens\u00edvel.","title":"Uso de Computa\u00e7\u00e3o Distribu\u00edda com Spark"},{"location":"datastorytelling/","text":"","title":"Data Storytelling"},{"location":"etl/","text":"","title":"Servi\u00e7os e ETLs para Transforma\u00e7\u00e3o de Dados e Processamento de Dados em Escala (train at scale)"},{"location":"ingestao/","text":"Ingest\u00e3o e Processamento de Dados \u00b6 Esta instru\u00e7\u00e3o foca em estrat\u00e9gias e metodologias essenciais para a ingest\u00e3o de dados em plataformas anal\u00edticas. Discutiremos as melhores pr\u00e1ticas para a coleta eficiente de dados de diversas fontes, a import\u00e2ncia de pipelines de dados confi\u00e1veis e como otimizar a ingest\u00e3o para an\u00e1lises em tempo real. Os alunos aprender\u00e3o sobre as tecnologias e ferramentas atuais que facilitam o processo de ingest\u00e3o, garantindo a qualidade e a disponibilidade dos dados para processamento subsequente e an\u00e1lise. Ser\u00e3o feitos scripts com testes integrados. Autoestudo Link Pontua\u00e7\u00e3o Pacote de Ingest\u00e3o de Dados Acesse 3 pontos Consultas complexas em SQL Acesse 0 pontos Agenda \u00b6 Atividade Tempo Demonstra\u00e7\u00e3o Pr\u00e1tica do Pacote de Ingest\u00e3o 60m Ponderada em Sala de Aula 60m Demonstra\u00e7\u00e3o Pr\u00e1tica de Um Pacote Python \u00b6 Pr\u00e9-requisitos \u00b6 Conhecimento b\u00e1sico em Python e Docker. Instala\u00e7\u00e3o do Docker e Docker Compose. Instala\u00e7\u00e3o do Poetry. Criando o Pacote \u00b6 poetry new data_pipeline Pyproject.toml \u00b6 cd data_pipeline poetry add flask requests pandas pyarrow minio clickhouse-connect python-dotenv Configurar o Docker Compose com MinIO e ClickHouse \u00b6 version : '3.8' services : minio : image : minio / minio : latest container_name : minio ports : - \"9000:9000\" # Porta para a API - \"9001:9001\" # Porta para a Web UI environment : MINIO_ROOT_USER : minioadmin MINIO_ROOT_PASSWORD : minioadmin volumes : - minio_data : / data # Persist\u00eancia de dados command : server / data -- console - address \":9001\" restart : always clickhouse : image : yandex / clickhouse - server : latest container_name : clickhouse ports : - \"8123:8123\" # Porta para a HTTP interface volumes : - clickhouse_data : / var / lib / clickhouse # Persist\u00eancia de dados restart : always volumes : minio_data : driver : local clickhouse_data : driver : local docker-compose up --build Vamos testar o Mini.IO \u00b6 from minio import Minio from minio.error import S3Error def test_minio (): # Configura\u00e7\u00e3o do cliente MinIO client = Minio ( \"localhost:9000\" , # Endere\u00e7o do MinIO access_key = \"minioadmin\" , # Chave de acesso secret_key = \"minioadmin\" , # Chave secreta secure = False # Use True se estiver usando HTTPS ) bucket_name = \"test-bucket\" object_name = \"test.txt\" file_content = \"Este \u00e9 um arquivo de teste.\" try : # Criar bucket se n\u00e3o existir if not client . bucket_exists ( bucket_name ): client . make_bucket ( bucket_name ) print ( f \"Bucket ' { bucket_name } ' criado com sucesso.\" ) else : print ( f \"Bucket ' { bucket_name } ' j\u00e1 existe.\" ) # Criar um arquivo de teste with open ( object_name , \"w\" ) as file : file . write ( file_content ) # Fazer upload do arquivo para o MinIO client . fput_object ( bucket_name , object_name , object_name ) print ( f \"Arquivo ' { object_name } ' enviado com sucesso para o bucket ' { bucket_name } '.\" ) # Fazer download do arquivo do MinIO client . fget_object ( bucket_name , object_name , f \"downloaded_ { object_name } \" ) print ( f \"Arquivo ' { object_name } ' baixado com sucesso do bucket ' { bucket_name } '.\" ) # Ler o conte\u00fado do arquivo baixado with open ( f \"downloaded_ { object_name } \" , \"r\" ) as file : downloaded_content = file . read () print ( f \"Conte\u00fado do arquivo baixado: { downloaded_content } \" ) except S3Error as e : print ( f \"Erro ao interagir com o MinIO: { e } \" ) if __name__ == \"__main__\" : test_minio () SQL para cra\u00e7\u00e3o da tabela! \u00b6 Crie uma pasta sql/ e o arquivo create_table.sql com o c\u00f3digo: CREATE TABLE IF NOT EXISTS working_data ( data_ingestao DateTime , dado_linha String , tag String ) ENGINE = MergeTree () ORDER BY data_ingestao ; Aplicativo Pipe de Dados \u00b6 Dentro da pasta data_pipeline crie os arquivos: clickhouse_client.py import clickhouse_connect import os from dotenv import load_dotenv load_dotenv () # Configura\u00e7\u00e3o do cliente ClickHouse CLICKHOUSE_HOST = os . getenv ( 'CLICKHOUSE_HOST' ) CLICKHOUSE_PORT = os . getenv ( 'CLICKHOUSE_PORT' ) def get_client (): return clickhouse_connect . get_client ( host = CLICKHOUSE_HOST , port = CLICKHOUSE_PORT ) def execute_sql_script ( script_path ): client = get_client () with open ( script_path , 'r' ) as file : sql_script = file . read () client . command ( sql_script ) return client def insert_dataframe ( client , table_name , df ): client . insert_df ( table_name , df ) data_processing.py import pandas as pd import pyarrow as pa import pyarrow.parquet as pq from datetime import datetime def process_data ( data ): # Criar DataFrame e salvar como Parquet df = pd . DataFrame ([ data ]) filename = f \"raw_data_ { datetime . now () . strftime ( '%Y%m %d %H%M%S' ) } .parquet\" table = pa . Table . from_pandas ( df ) pq . write_table ( table , filename ) return filename def prepare_dataframe_for_insert ( df ): df [ 'data_ingestao' ] = datetime . now () df [ 'dado_linha' ] = df . apply ( lambda row : row . to_json (), axis = 1 ) df [ 'tag' ] = 'example_tag' return df [[ 'data_ingestao' , 'dado_linha' , 'tag' ]] minio_client.py from minio import Minio import os from dotenv import load_dotenv load_dotenv () # Carregar vari\u00e1veis de ambiente MINIO_ENDPOINT = os . getenv ( 'MINIO_ENDPOINT' ) MINIO_ACCESS_KEY = os . getenv ( 'MINIO_ACCESS_KEY' ) MINIO_SECRET_KEY = os . getenv ( 'MINIO_SECRET_KEY' ) # Configura\u00e7\u00e3o do cliente MinIO minio_client = Minio ( MINIO_ENDPOINT , access_key = MINIO_ACCESS_KEY , secret_key = MINIO_SECRET_KEY , secure = False ) def create_bucket_if_not_exists ( bucket_name ): if not minio_client . bucket_exists ( bucket_name ): minio_client . make_bucket ( bucket_name ) def upload_file ( bucket_name , file_path ): file_name = os . path . basename ( file_path ) minio_client . fput_object ( bucket_name , file_name , file_path ) def download_file ( bucket_name , file_name , local_file_path ): minio_client . fget_object ( bucket_name , file_name , local_file_path ) e na pasta raiz o app.py from flask import Flask , request , jsonify from datetime import datetime from data_pipeline.minio_client import create_bucket_if_not_exists , upload_file , download_file from data_pipeline.clickhouse_client import execute_sql_script , get_client , insert_dataframe from data_pipeline.data_processing import process_data , prepare_dataframe_for_insert import pandas as pd app = Flask ( __name__ ) # Criar bucket se n\u00e3o existir create_bucket_if_not_exists ( \"raw-data\" ) # Executar o script SQL para criar a tabela execute_sql_script ( 'sql/create_table.sql' ) @app . route ( '/data' , methods = [ 'POST' ]) def receive_data (): data = request . get_json () if not data or 'date' not in data or 'dados' not in data : return jsonify ({ \"error\" : \"Formato de dados inv\u00e1lido\" }), 400 try : datetime . fromtimestamp ( data [ 'date' ]) int ( data [ 'dados' ]) except ( ValueError , TypeError ): return jsonify ({ \"error\" : \"Tipo de dados inv\u00e1lido\" }), 400 # Processar e salvar dados filename = process_data ( data ) upload_file ( \"raw-data\" , filename ) # Ler arquivo Parquet do MinIO download_file ( \"raw-data\" , filename , f \"downloaded_ { filename } \" ) df_parquet = pd . read_parquet ( f \"downloaded_ { filename } \" ) # Preparar e inserir dados no ClickHouse df_prepared = prepare_dataframe_for_insert ( df_parquet ) client = get_client () # Obter o cliente ClickHouse insert_dataframe ( client , 'working_data' , df_prepared ) return jsonify ({ \"message\" : \"Dados recebidos, armazenados e processados com sucesso\" }), 200 if __name__ == '__main__' : app . run ( host = '0.0.0.0' , port = 5000 ) e o .env MINIO_ENDPOINT=localhost:9000 MINIO_ACCESS_KEY=minioadmin MINIO_SECRET_KEY=minioadmin CLICKHOUSE_HOST=localhost CLICKHOUSE_PORT=8123 Vamos testar com requests.http \u00b6 POST http://localhost:5000/data Content-Type: application/json { \"date\": 1692345600, \"dados\": 12345 } Agora criamos a VIEW: \u00b6 CREATE VIEW IF NOT EXISTS working_data_view AS SELECT data_ingestao , JSONExtractInt ( dado_linha , 'date' ) AS date_unix , JSONExtractInt ( dado_linha , 'dados' ) AS dados , toDateTime ( JSONExtractInt ( dado_linha , 'data_ingestao' ) / 1000 ) AS data_ingestao_datetime FROM working_data ; Este \u00e9 um pacote de demonstra\u00e7\u00e3o: seu grupo pode e deve criar uma arquitetura pr\u00f3pria com Testes! \u00b6 Tarefa \u00b6 Fazer em sala de aula a Ponderada!","title":"Ingest\u00e3o e Processamento de Dados (Framework de testes e qualidade)"},{"location":"ingestao/#ingestao-e-processamento-de-dados","text":"Esta instru\u00e7\u00e3o foca em estrat\u00e9gias e metodologias essenciais para a ingest\u00e3o de dados em plataformas anal\u00edticas. Discutiremos as melhores pr\u00e1ticas para a coleta eficiente de dados de diversas fontes, a import\u00e2ncia de pipelines de dados confi\u00e1veis e como otimizar a ingest\u00e3o para an\u00e1lises em tempo real. Os alunos aprender\u00e3o sobre as tecnologias e ferramentas atuais que facilitam o processo de ingest\u00e3o, garantindo a qualidade e a disponibilidade dos dados para processamento subsequente e an\u00e1lise. Ser\u00e3o feitos scripts com testes integrados. Autoestudo Link Pontua\u00e7\u00e3o Pacote de Ingest\u00e3o de Dados Acesse 3 pontos Consultas complexas em SQL Acesse 0 pontos","title":"Ingest\u00e3o e Processamento de Dados"},{"location":"ingestao/#agenda","text":"Atividade Tempo Demonstra\u00e7\u00e3o Pr\u00e1tica do Pacote de Ingest\u00e3o 60m Ponderada em Sala de Aula 60m","title":"Agenda"},{"location":"ingestao/#demonstracao-pratica-de-um-pacote-python","text":"","title":"Demonstra\u00e7\u00e3o Pr\u00e1tica de Um Pacote Python"},{"location":"ingestao/#pre-requisitos","text":"Conhecimento b\u00e1sico em Python e Docker. Instala\u00e7\u00e3o do Docker e Docker Compose. Instala\u00e7\u00e3o do Poetry.","title":"Pr\u00e9-requisitos"},{"location":"ingestao/#criando-o-pacote","text":"poetry new data_pipeline","title":"Criando o Pacote"},{"location":"ingestao/#pyprojecttoml","text":"cd data_pipeline poetry add flask requests pandas pyarrow minio clickhouse-connect python-dotenv","title":"Pyproject.toml"},{"location":"ingestao/#configurar-o-docker-compose-com-minio-e-clickhouse","text":"version : '3.8' services : minio : image : minio / minio : latest container_name : minio ports : - \"9000:9000\" # Porta para a API - \"9001:9001\" # Porta para a Web UI environment : MINIO_ROOT_USER : minioadmin MINIO_ROOT_PASSWORD : minioadmin volumes : - minio_data : / data # Persist\u00eancia de dados command : server / data -- console - address \":9001\" restart : always clickhouse : image : yandex / clickhouse - server : latest container_name : clickhouse ports : - \"8123:8123\" # Porta para a HTTP interface volumes : - clickhouse_data : / var / lib / clickhouse # Persist\u00eancia de dados restart : always volumes : minio_data : driver : local clickhouse_data : driver : local docker-compose up --build","title":"Configurar o Docker Compose com MinIO e ClickHouse"},{"location":"ingestao/#vamos-testar-o-miniio","text":"from minio import Minio from minio.error import S3Error def test_minio (): # Configura\u00e7\u00e3o do cliente MinIO client = Minio ( \"localhost:9000\" , # Endere\u00e7o do MinIO access_key = \"minioadmin\" , # Chave de acesso secret_key = \"minioadmin\" , # Chave secreta secure = False # Use True se estiver usando HTTPS ) bucket_name = \"test-bucket\" object_name = \"test.txt\" file_content = \"Este \u00e9 um arquivo de teste.\" try : # Criar bucket se n\u00e3o existir if not client . bucket_exists ( bucket_name ): client . make_bucket ( bucket_name ) print ( f \"Bucket ' { bucket_name } ' criado com sucesso.\" ) else : print ( f \"Bucket ' { bucket_name } ' j\u00e1 existe.\" ) # Criar um arquivo de teste with open ( object_name , \"w\" ) as file : file . write ( file_content ) # Fazer upload do arquivo para o MinIO client . fput_object ( bucket_name , object_name , object_name ) print ( f \"Arquivo ' { object_name } ' enviado com sucesso para o bucket ' { bucket_name } '.\" ) # Fazer download do arquivo do MinIO client . fget_object ( bucket_name , object_name , f \"downloaded_ { object_name } \" ) print ( f \"Arquivo ' { object_name } ' baixado com sucesso do bucket ' { bucket_name } '.\" ) # Ler o conte\u00fado do arquivo baixado with open ( f \"downloaded_ { object_name } \" , \"r\" ) as file : downloaded_content = file . read () print ( f \"Conte\u00fado do arquivo baixado: { downloaded_content } \" ) except S3Error as e : print ( f \"Erro ao interagir com o MinIO: { e } \" ) if __name__ == \"__main__\" : test_minio ()","title":"Vamos testar o Mini.IO"},{"location":"ingestao/#sql-para-cracao-da-tabela","text":"Crie uma pasta sql/ e o arquivo create_table.sql com o c\u00f3digo: CREATE TABLE IF NOT EXISTS working_data ( data_ingestao DateTime , dado_linha String , tag String ) ENGINE = MergeTree () ORDER BY data_ingestao ;","title":"SQL para cra\u00e7\u00e3o da tabela!"},{"location":"ingestao/#aplicativo-pipe-de-dados","text":"Dentro da pasta data_pipeline crie os arquivos: clickhouse_client.py import clickhouse_connect import os from dotenv import load_dotenv load_dotenv () # Configura\u00e7\u00e3o do cliente ClickHouse CLICKHOUSE_HOST = os . getenv ( 'CLICKHOUSE_HOST' ) CLICKHOUSE_PORT = os . getenv ( 'CLICKHOUSE_PORT' ) def get_client (): return clickhouse_connect . get_client ( host = CLICKHOUSE_HOST , port = CLICKHOUSE_PORT ) def execute_sql_script ( script_path ): client = get_client () with open ( script_path , 'r' ) as file : sql_script = file . read () client . command ( sql_script ) return client def insert_dataframe ( client , table_name , df ): client . insert_df ( table_name , df ) data_processing.py import pandas as pd import pyarrow as pa import pyarrow.parquet as pq from datetime import datetime def process_data ( data ): # Criar DataFrame e salvar como Parquet df = pd . DataFrame ([ data ]) filename = f \"raw_data_ { datetime . now () . strftime ( '%Y%m %d %H%M%S' ) } .parquet\" table = pa . Table . from_pandas ( df ) pq . write_table ( table , filename ) return filename def prepare_dataframe_for_insert ( df ): df [ 'data_ingestao' ] = datetime . now () df [ 'dado_linha' ] = df . apply ( lambda row : row . to_json (), axis = 1 ) df [ 'tag' ] = 'example_tag' return df [[ 'data_ingestao' , 'dado_linha' , 'tag' ]] minio_client.py from minio import Minio import os from dotenv import load_dotenv load_dotenv () # Carregar vari\u00e1veis de ambiente MINIO_ENDPOINT = os . getenv ( 'MINIO_ENDPOINT' ) MINIO_ACCESS_KEY = os . getenv ( 'MINIO_ACCESS_KEY' ) MINIO_SECRET_KEY = os . getenv ( 'MINIO_SECRET_KEY' ) # Configura\u00e7\u00e3o do cliente MinIO minio_client = Minio ( MINIO_ENDPOINT , access_key = MINIO_ACCESS_KEY , secret_key = MINIO_SECRET_KEY , secure = False ) def create_bucket_if_not_exists ( bucket_name ): if not minio_client . bucket_exists ( bucket_name ): minio_client . make_bucket ( bucket_name ) def upload_file ( bucket_name , file_path ): file_name = os . path . basename ( file_path ) minio_client . fput_object ( bucket_name , file_name , file_path ) def download_file ( bucket_name , file_name , local_file_path ): minio_client . fget_object ( bucket_name , file_name , local_file_path ) e na pasta raiz o app.py from flask import Flask , request , jsonify from datetime import datetime from data_pipeline.minio_client import create_bucket_if_not_exists , upload_file , download_file from data_pipeline.clickhouse_client import execute_sql_script , get_client , insert_dataframe from data_pipeline.data_processing import process_data , prepare_dataframe_for_insert import pandas as pd app = Flask ( __name__ ) # Criar bucket se n\u00e3o existir create_bucket_if_not_exists ( \"raw-data\" ) # Executar o script SQL para criar a tabela execute_sql_script ( 'sql/create_table.sql' ) @app . route ( '/data' , methods = [ 'POST' ]) def receive_data (): data = request . get_json () if not data or 'date' not in data or 'dados' not in data : return jsonify ({ \"error\" : \"Formato de dados inv\u00e1lido\" }), 400 try : datetime . fromtimestamp ( data [ 'date' ]) int ( data [ 'dados' ]) except ( ValueError , TypeError ): return jsonify ({ \"error\" : \"Tipo de dados inv\u00e1lido\" }), 400 # Processar e salvar dados filename = process_data ( data ) upload_file ( \"raw-data\" , filename ) # Ler arquivo Parquet do MinIO download_file ( \"raw-data\" , filename , f \"downloaded_ { filename } \" ) df_parquet = pd . read_parquet ( f \"downloaded_ { filename } \" ) # Preparar e inserir dados no ClickHouse df_prepared = prepare_dataframe_for_insert ( df_parquet ) client = get_client () # Obter o cliente ClickHouse insert_dataframe ( client , 'working_data' , df_prepared ) return jsonify ({ \"message\" : \"Dados recebidos, armazenados e processados com sucesso\" }), 200 if __name__ == '__main__' : app . run ( host = '0.0.0.0' , port = 5000 ) e o .env MINIO_ENDPOINT=localhost:9000 MINIO_ACCESS_KEY=minioadmin MINIO_SECRET_KEY=minioadmin CLICKHOUSE_HOST=localhost CLICKHOUSE_PORT=8123","title":"Aplicativo Pipe de Dados"},{"location":"ingestao/#vamos-testar-com-requestshttp","text":"POST http://localhost:5000/data Content-Type: application/json { \"date\": 1692345600, \"dados\": 12345 }","title":"Vamos testar com requests.http"},{"location":"ingestao/#agora-criamos-a-view","text":"CREATE VIEW IF NOT EXISTS working_data_view AS SELECT data_ingestao , JSONExtractInt ( dado_linha , 'date' ) AS date_unix , JSONExtractInt ( dado_linha , 'dados' ) AS dados , toDateTime ( JSONExtractInt ( dado_linha , 'data_ingestao' ) / 1000 ) AS data_ingestao_datetime FROM working_data ;","title":"Agora criamos a VIEW:"},{"location":"ingestao/#este-e-um-pacote-de-demonstracao-seu-grupo-pode-e-deve-criar-uma-arquitetura-propria-com-testes","text":"","title":"Este \u00e9 um pacote de demonstra\u00e7\u00e3o: seu grupo pode e deve criar uma arquitetura pr\u00f3pria com Testes!"},{"location":"ingestao/#tarefa","text":"Fazer em sala de aula a Ponderada!","title":"Tarefa"},{"location":"metricas/","text":"","title":"M\u00e9tricas, Telemetria e Gest\u00e3o de Microservi\u00e7os"},{"location":"mlops/","text":"","title":"Ciclo de Vida e Automatiza\u00e7\u00e3o de Pipeline de Dados (MLOps)"},{"location":"precificacao/","text":"","title":"Precifica\u00e7\u00e3o em Nuvem"},{"location":"predicao/","text":"","title":"Processamento e Predi\u00e7\u00e3o em Produ\u00e7\u00e3o"},{"location":"processamento/","text":"","title":"Processamento de Dados na Nuvem"},{"location":"seguranca/","text":"","title":"Seguran\u00e7a e Governan\u00e7a de Dados (LGPD)"},{"location":"ui_dashboards/","text":"","title":"User Interface e Dashboards"}]}